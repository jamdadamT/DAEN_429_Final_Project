{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b6277ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b6277ca",
        "outputId": "4016cb5a-a4f4-47db-83e0-40b1fd0ab4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard PyTorch + Torchvision stack\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Reproducibility (essential for research and debugging)\n",
        "import random\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Note: For complete reproducibility, you may also need:\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device (GPU if available)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8uD6Fg9ciL",
        "outputId": "2228cdfd-da0c-4c08-9270-517d66679609"
      },
      "id": "KJ8uD6Fg9ciL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1c593ca7",
      "metadata": {
        "id": "1c593ca7"
      },
      "outputs": [],
      "source": [
        "# Let's examine a fresh ResNet-18 pretrained on ImageNet\n",
        "res18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c363e3",
      "metadata": {
        "id": "88c363e3"
      },
      "source": [
        "## 3) Why Transfer Learning? The Power of Pretrained Features\n",
        "\n",
        "### The Transfer Learning Hypothesis\n",
        "\n",
        "Networks trained on large datasets (like ImageNet with 1.2M images, 1000 classes) learn hierarchical features:\n",
        "\n",
        "Early layers (conv1, layer1): Low-level features (edges, textures, colors)\n",
        "\n",
        "Middle layers (layer2, layer3): Mid-level features (shapes, parts, patterns)\n",
        "\n",
        "Deep layers (layer4): High-level, task-specific features (object parts)\n",
        "\n",
        "Final layer (fc): Class-specific decision boundaries\n",
        "\n",
        "Key Insight: Low and mid-level features are universal across vision tasks! We can reuse them and only adapt the high-level features to our new task.\n",
        "\n",
        "## Fine-Tuning Strategies\n",
        "\n",
        "### 1) Feature Extraction *(Freeze all, train head)*\n",
        "- **Pros:** Fastest; lowest overfitting risk  \n",
        "- **Use when:** Limited data; domain ≈ ImageNet  \n",
        "- **Unfrozen:** `fc` only\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Shallow Fine-Tuning *(Unfreeze layer4 + head)*\n",
        "- **Pros:** Adapts high-level features; still efficient  \n",
        "- **Use when:** Moderate data; somewhat different domain  \n",
        "- **Unfrozen:** `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Deep Fine-Tuning *(Unfreeze layer3 + layer4 + head)*\n",
        "- **Pros:** Greater adaptation capacity  \n",
        "- **Use when:** Sufficient data; noticeable domain shift  \n",
        "- **Unfrozen:** `layer3`, `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Fine-Tuning *(Unfreeze everything)*\n",
        "- **Pros:** Maximum flexibility  \n",
        "- **Cons:** Slowest; higher overfitting risk  \n",
        "- **Use when:** Large dataset; very different domain  \n",
        "- **Unfrozen:** all layers\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Tips\n",
        "- Prefer **smaller LR** for earlier layers (discriminative LRs).\n",
        "- Add regularization when unfreezing more (augmentations, weight decay, label smoothing).\n",
        "- Monitor validation; consider early stopping/checkpointing.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c1c800",
      "metadata": {
        "id": "56c1c800"
      },
      "source": [
        "## 4) Data Preprocessing: Why ImageNet Statistics?\n",
        "\n",
        "### Understanding ImageNet Normalization\n",
        "Pretrained networks expect inputs with specific statistics because they were trained on normalized ImageNet data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6c7c62b",
      "metadata": {
        "id": "c6c7c62b"
      },
      "outputs": [],
      "source": [
        "# ImageNet channel-wise statistics (computed over millions of images)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # Mean per channel (R, G, B)\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]  # Std dev per channel\n",
        "\n",
        "# Why these specific values?\n",
        "# - They center the data around 0 and scale to ~[-2, 2] range\n",
        "# - This matches the distribution the network was trained on\n",
        "# - Network weights are calibrated to these input scales"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRGFc2kLOCov",
        "outputId": "cb7f614d-065a-4512-ecf2-a6ac7163a3c5"
      },
      "id": "HRGFc2kLOCov",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'asl-alphabet' dataset.\n",
            "Path to dataset files: /kaggle/input/asl-alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "IBl1Lx9rMSec",
        "outputId": "b8825f5e-b3f2-434f-b5ed-b271bf3228f7"
      },
      "id": "IBl1Lx9rMSec",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fe19794c-9767-4801-af4b-e96e4f0c0d44\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fe19794c-9767-4801-af4b-e96e4f0c0d44\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1613494533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    174\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Train_Data.zip -d /content/"
      ],
      "metadata": {
        "id": "gDASgn89HPk0"
      },
      "id": "gDASgn89HPk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\" /content/"
      ],
      "metadata": {
        "id": "cCeYJpjaHT8j"
      },
      "id": "cCeYJpjaHT8j",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0b79974f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b79974f",
        "outputId": "7e7c86a7-c3a5-49f9-83b9-036f788d01c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "in between\n",
            "here\n",
            " Dataset: 87,000 train, 28 val\n",
            " Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] and test classes ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224          # Standard ImageNet size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Training transforms: Add variability to prevent overfitting\n",
        "train_tf = transforms.Compose([\n",
        "    # 1. Resize: ASL images need to be 224×224\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "\n",
        "    # 3. Convert to tensor: PIL Image → Tensor, scales to [0,1]\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # 4. Normalize: Match ImageNet statistics\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    # This does: output = (input - mean) / std\n",
        "])\n",
        "\n",
        "# Validation transforms: No augmentation (we want consistent evaluation)\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "print('here')\n",
        "# Load ASL dataset\n",
        "full_train_ds = datasets.ImageFolder(root=\"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\",transform=train_tf)\n",
        "print('in between')\n",
        "given_test_ds = datasets.ImageFolder(root=\"/content/Given_Test\",transform=val_tf)\n",
        "\n",
        "#full_train_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Train_Data\", transform=train_tf)\n",
        "#given_test_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\",transform=val_tf)\n",
        "\n",
        "print('here')\n",
        "#Need to split full train into validation set\n",
        "indices = np.arange(len( full_train_ds ) )\n",
        "labels = np.array(full_train_ds.targets ) # ASL labels\n",
        "train_idx , val_idx = train_test_split(indices , test_size =0.2 , stratify = labels , random_state =429)\n",
        "\n",
        "\n",
        "train_subset_ds = Subset(full_train_ds, train_idx)\n",
        "\n",
        "# Create the stratified Validation Subset\n",
        "val_subset_ds = Subset(full_train_ds, val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    given_test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "NUM_CLASSES = 29\n",
        "print(f' Dataset: {len(full_train_ds):,} train, {len(given_test_ds):,} val')\n",
        "print(f' Classes: {full_train_ds.classes} and test classes {given_test_ds.classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e90d56c0",
      "metadata": {
        "id": "e90d56c0"
      },
      "source": [
        "## 5) Model Setup: Adapting ResNet-18 for ASL Translation\n",
        "\n",
        "Replacing the Classification Head\n",
        "\n",
        "The pretrained ResNet-18 outputs 1000 classes (ImageNet), but we need 28 (ASL no del):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b5bde5ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bde5ef",
        "outputId": "642e2d22-ad21-456a-d2b3-de5be0110728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 1000 (ImageNet classes)\n",
            "\n",
            " New FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 29 (our classes)\n"
          ]
        }
      ],
      "source": [
        "# Start with ImageNet-pretrained weights\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Examine the original classifier\n",
        "print(\" Original FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (ImageNet classes)\")\n",
        "\n",
        "# Replace with our custom classifier\n",
        "# The in_features must match (512 for ResNet-18's final feature size)\n",
        "# The NUM_CLASSES will change for other datasets\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "print(\"\\n New FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (our classes)\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a03d7b",
      "metadata": {
        "id": "11a03d7b"
      },
      "source": [
        "## Understanding Parameter Names and Hierarchy\n",
        "To selectively freeze/unfreeze layers, we need to understand PyTorch's parameter naming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0b8ca81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b8ca81",
        "outputId": "0a8ee56d-7809-48a9-b131-4c69b5bf8ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Structure (hierarchical view):\n",
            "├── conv1: Conv2d (9,408 params, 9,408 trainable)\n",
            "├── bn1: BatchNorm2d (128 params, 128 trainable)\n",
            "├── relu: ReLU (0 params, 0 trainable)\n",
            "├── maxpool: MaxPool2d (0 params, 0 trainable)\n",
            "├── layer1: Sequential (147,968 params, 147,968 trainable)\n",
            "│   ├── 0: BasicBlock (73,984 params, 73,984 trainable)\n",
            "│   ├── 1: BasicBlock (73,984 params, 73,984 trainable)\n",
            "├── layer2: Sequential (525,568 params, 525,568 trainable)\n",
            "│   ├── 0: BasicBlock (230,144 params, 230,144 trainable)\n",
            "│   ├── 1: BasicBlock (295,424 params, 295,424 trainable)\n",
            "├── layer3: Sequential (2,099,712 params, 2,099,712 trainable)\n",
            "│   ├── 0: BasicBlock (919,040 params, 919,040 trainable)\n",
            "│   ├── 1: BasicBlock (1,180,672 params, 1,180,672 trainable)\n",
            "├── layer4: Sequential (8,393,728 params, 8,393,728 trainable)\n",
            "│   ├── 0: BasicBlock (3,673,088 params, 3,673,088 trainable)\n",
            "│   ├── 1: BasicBlock (4,720,640 params, 4,720,640 trainable)\n",
            "├── avgpool: AdaptiveAvgPool2d (0 params, 0 trainable)\n",
            "├── fc: Linear (14,877 params, 14,877 trainable)\n"
          ]
        }
      ],
      "source": [
        "def explore_model_structure(model, max_depth=2):\n",
        "    \"\"\"Visualize the model's hierarchical structure\"\"\"\n",
        "\n",
        "    print(\"\\n Model Structure (hierarchical view):\")\n",
        "\n",
        "    def print_module(module, prefix=\"\", depth=0):\n",
        "        if depth >= max_depth:\n",
        "            return\n",
        "        for name, child in module.named_children():\n",
        "            param_count = sum(p.numel() for p in child.parameters())\n",
        "            trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
        "            print(f\"{prefix}├── {name}: {child.__class__.__name__} \"\n",
        "                  f\"({param_count:,} params, {trainable:,} trainable)\")\n",
        "            if depth < max_depth - 1:\n",
        "                print_module(child, prefix + \"│   \", depth + 1)\n",
        "\n",
        "    print_module(model)\n",
        "\n",
        "# Explore structure\n",
        "explore_model_structure(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c35761",
      "metadata": {
        "id": "d2c35761"
      },
      "source": [
        "## 6) Freezing and Unfreezing: The Core Mechanism\n",
        "### How Freezing Works\n",
        "When we \"freeze\" a layer, we set requires_grad=False on its parameters:\n",
        "\n",
        "Frozen parameters: No gradients computed, no updates during backprop\n",
        "\n",
        "Unfrozen parameters: Gradients computed, weights updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9c5647d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c5647d8",
        "outputId": "80737e9c-c22a-4b93-cdb7-e2d1b036d0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Freezing entire model...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing only the FC layer...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Trainable: 14,877 / 11,191,389 parameters (0.132933%)\n"
          ]
        }
      ],
      "source": [
        "def set_requires_grad(module: nn.Module, requires_grad: bool):\n",
        "    \"\"\"\n",
        "    Recursively set requires_grad for all parameters in a module.\n",
        "\n",
        "    Args:\n",
        "        module: PyTorch module (layer, block, or entire model)\n",
        "        requires_grad: True to unfreeze (train), False to freeze\n",
        "    \"\"\"\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "    # Print status\n",
        "    param_count = sum(p.numel() for p in module.parameters())\n",
        "    status = \"UNFROZEN (trainable)\" if requires_grad else \"FROZEN\"\n",
        "    print(f\"  {module.__class__.__name__}: {param_count:,} parameters {status}\")\n",
        "\n",
        "# Example: Freeze entire model, then selectively unfreeze\n",
        "print(\" Freezing entire model...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "print(\"\\n Unfreezing only the FC layer...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "\n",
        "# Verify what's trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n Trainable: {trainable_params:,} / {total_params:,} parameters \"\n",
        "      f\"({100*trainable_params/total_params:.6f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ee588b",
      "metadata": {
        "id": "36ee588b"
      },
      "source": [
        "## 7) Training Infrastructure\n",
        "\n",
        "Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "63addda7",
      "metadata": {
        "id": "63addda7"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout, batch norm training mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        # Optional: Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"    Batch {batch_idx}/{len(loader)}, \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "        #print('Here 1')\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()  # Decorator disables gradient computation\n",
        "def evaluate(model, loader):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation/test set.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    print('Reached Evaluate')\n",
        "    model.eval()  # Disable dropout, batch norm eval mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "    all_preds = [] # <-- Collects all predicted labels\n",
        "    all_targets = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass only (no backward)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "\n",
        "        all_preds.extend(predictions.cpu().tolist())\n",
        "        # FIX HERE: Use 'labels' instead of 'targets'\n",
        "        all_targets.extend(labels.cpu().tolist())\n",
        "\n",
        "\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy, all_targets, all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d5930662",
      "metadata": {
        "id": "d5930662"
      },
      "outputs": [],
      "source": [
        "#Define dictionary that is going to hold the loss accross epochs for each model for comparison purposes\n",
        "total_loss_by_epoch_model = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142d945e",
      "metadata": {
        "id": "142d945e"
      },
      "source": [
        "## 8) Phase 1.1: Head-Only Fine-Tuning (Feature Extraction)\n",
        "Strategy: Use ResNet as a Fixed Feature Extractor\n",
        "\n",
        "In this phase, we:\n",
        "\n",
        "1. Freeze all convolutional layers (keep ImageNet features)\n",
        "\n",
        "2. Train only the new classifier head (learn new class boundaries)\n",
        "\n",
        "3. Use higher learning rate (since we're training from scratch)\n",
        "\n",
        "This is the safest approach with limited data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "78d0ce38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78d0ce38",
        "outputId": "31c22437-24b4-4a3f-c8b8-c7ebe67f7a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/1088, Loss: 2.6674\n",
            "    Batch 100/1088, Loss: 1.4809\n",
            "    Batch 200/1088, Loss: 0.9902\n",
            "    Batch 300/1088, Loss: 0.7204\n",
            "    Batch 400/1088, Loss: 0.5434\n",
            "    Batch 500/1088, Loss: 0.5824\n",
            "    Batch 600/1088, Loss: 0.4506\n",
            "    Batch 700/1088, Loss: 0.5051\n",
            "    Batch 800/1088, Loss: 0.4005\n",
            "    Batch 900/1088, Loss: 0.4230\n",
            "    Batch 1000/1088, Loss: 0.4672\n",
            "   Train: Loss=0.7441, Acc=0.856\n",
            "   Val:   Loss=0.3098, Acc=0.941  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/1088, Loss: 0.3416\n",
            "    Batch 100/1088, Loss: 0.3186\n",
            "    Batch 200/1088, Loss: 0.2835\n",
            "    Batch 300/1088, Loss: 0.2847\n",
            "    Batch 400/1088, Loss: 0.3211\n",
            "    Batch 500/1088, Loss: 0.3403\n",
            "    Batch 600/1088, Loss: 0.2550\n",
            "    Batch 700/1088, Loss: 0.2598\n",
            "    Batch 800/1088, Loss: 0.2506\n",
            "    Batch 900/1088, Loss: 0.2804\n",
            "    Batch 1000/1088, Loss: 0.2138\n",
            "   Train: Loss=0.2668, Acc=0.943\n",
            "   Val:   Loss=0.1847, Acc=0.963  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/1088, Loss: 0.1650\n",
            "    Batch 100/1088, Loss: 0.2375\n",
            "    Batch 200/1088, Loss: 0.2026\n",
            "    Batch 300/1088, Loss: 0.1654\n",
            "    Batch 400/1088, Loss: 0.1715\n",
            "    Batch 500/1088, Loss: 0.1694\n",
            "    Batch 600/1088, Loss: 0.2497\n",
            "    Batch 700/1088, Loss: 0.1853\n",
            "    Batch 800/1088, Loss: 0.3495\n",
            "    Batch 900/1088, Loss: 0.1292\n",
            "    Batch 1000/1088, Loss: 0.1470\n",
            "   Train: Loss=0.1817, Acc=0.959\n",
            "   Val:   Loss=0.1313, Acc=0.974  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.974\n",
            "Macro-F1 Score (Best Epoch): 0.9736\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2295    7    0    0   44    0    0    0    1    0    1    1   10    0\n",
            "     0    0    0    0    9   19    0    0    0    2    0   11    0    0\n",
            "     0]\n",
            " [   2 2362    0    1   11    0    0    0    3    0    4    0    2    0\n",
            "     0    0    0    1    4    0    9    1    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0 2390    4    0    0    0    0    0    1    0    0    0    0\n",
            "     1    0    1    0    2    0    0    0    0    0    0    1    0    0\n",
            "     0]\n",
            " [   0    0    1 2388    2    2    0    0    0    0    0    0    0    0\n",
            "     3    1    0    0    0    0    2    0    0    0    0    0    0    0\n",
            "     1]\n",
            " [   5    1    0    0 2374    0    0    0    4    0    0    0    2    1\n",
            "     0    0    0    0    8    0    2    0    0    1    0    2    0    0\n",
            "     0]\n",
            " [   3    1    0    0    2 2379    0    0    1    4    0    1    0    0\n",
            "     0    0    0    0    0    3    0    4    2    0    0    0    0    0\n",
            "     0]\n",
            " [   3    3    0    0    4    1 2339   11    0   13    5    0    2    0\n",
            "     0    0    0    0    2    0    3    0    1    1    5    3    1    0\n",
            "     3]\n",
            " [   0    0    0    0    0    0    3 2394    0    1    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    1    0    0    1\n",
            "     0]\n",
            " [   1    1    0    1   19    0    0    0 2310    8   11    0    2    0\n",
            "     1    0    0    2   10    0   12    1    1   14    3    3    0    0\n",
            "     0]\n",
            " [   0    0    0    0    1    0    1    0    7 2382    0    0    0    1\n",
            "     0    0    0    2    4    0    0    0    1    0    0    0    1    0\n",
            "     0]\n",
            " [   0    0    0    1    2    0    2    1    3    1 2350    0    2    0\n",
            "     0    0    0    5    2    0    4   13   11    2    0    1    0    0\n",
            "     0]\n",
            " [   3    0    0    0    0    0    0    0    0    1    3 2375    0    0\n",
            "     0    0    0    0    0    9    2    0    1    2    0    4    0    0\n",
            "     0]\n",
            " [   0    1    0    0    0    0    0    0    0    0    0    0 2360   28\n",
            "     0    0    0    0    5    1    1    0    0    3    0    1    0    0\n",
            "     0]\n",
            " [   0    2    0    0    0    0    1    0    0    0    0    0  100 2282\n",
            "     0    0    0    0    5    0    2    0    0    1    0    1    5    0\n",
            "     1]\n",
            " [   0    0    0    2    1    0    0    0    1    2    0    0    1    1\n",
            "  2364    1    0    0    9    0    1    0    1    5    0   11    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    1    0    1    0    0\n",
            "     0 2364   19    0    0    0    0    0    0    0    0    8    7    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    1    0    0    0    0    0    0\n",
            "     0    5 2387    0    0    0    0    0    0    0    0    3    4    0\n",
            "     0]\n",
            " [   1    1    0    0    0    0    0    0    7    2   14    0    1    1\n",
            "     1    0    0 2183    3    0  128   10   10   29    1    3    3    0\n",
            "     2]\n",
            " [   3    0    0    0   29    0    0    0    1    2    1    0    3    2\n",
            "     1    0    0    4 2216   13    9    0    2  105    0    9    0    0\n",
            "     0]\n",
            " [   4    0    0    0    3    0    0    0    1    0    0    4    0    0\n",
            "     1    0    0    0   18 2309    2    1    3   32   12    9    1    0\n",
            "     0]\n",
            " [   1    0    0    0    3    0    0    0    0    0    6    0    0    0\n",
            "     0    0    0   36    1    0 2283   18    7   37    1    7    0    0\n",
            "     0]\n",
            " [   0    0    0    2    3    1    0    0    4    0   27    1    0    0\n",
            "     0    0    0    6    3    0   55 2182   89   25    1    1    0    0\n",
            "     0]\n",
            " [   0    0    0    0    1    0    0    0    0    0    5    0    1    0\n",
            "     0    0    0    1    0    0    6   22 2362    1    0    1    0    0\n",
            "     0]\n",
            " [   0    1    0    0    4    0    0    0    0    2    0    2    0    4\n",
            "     0    0    0    6   22    8   32    7    1 2295    2   14    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    2    0    0    1    5    2    0    0\n",
            "     0    0    0    0    5   36    2    1   13   20 2280   33    0    0\n",
            "     0]\n",
            " [   0    0    0    0    2    0    0    0    0    1    0    0    0    0\n",
            "     0    0    0    0    1    4    0    4    0    2    1 2385    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    2    0    0    1    0\n",
            "     1    0    2    1    0    0    0    0    0    0    0    8 2385    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    1    0    0    0    0    1    0    0    1    0    0\n",
            "     0    0    0    6    1    0    1    0    0    5    2    0    1    0\n",
            "  2381]]\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.01\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Batch 0/544, Loss: 0.1302\n",
            "    Batch 100/544, Loss: 0.1012\n",
            "    Batch 200/544, Loss: 0.1566\n",
            "    Batch 300/544, Loss: 0.0885\n",
            "    Batch 400/544, Loss: 0.1313\n",
            "    Batch 500/544, Loss: 0.1392\n",
            "   Train: Loss=0.1318, Acc=0.975\n",
            "   Val:   Loss=0.1209, Acc=0.978  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/544, Loss: 0.1241\n",
            "    Batch 100/544, Loss: 0.1378\n",
            "    Batch 200/544, Loss: 0.1993\n",
            "    Batch 300/544, Loss: 0.1158\n",
            "    Batch 400/544, Loss: 0.1166\n",
            "    Batch 500/544, Loss: 0.1436\n",
            "   Train: Loss=0.1294, Acc=0.976\n",
            "   Val:   Loss=0.1185, Acc=0.978  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/544, Loss: 0.1183\n",
            "    Batch 100/544, Loss: 0.1886\n",
            "    Batch 200/544, Loss: 0.1172\n",
            "    Batch 300/544, Loss: 0.1059\n",
            "    Batch 400/544, Loss: 0.1160\n",
            "    Batch 500/544, Loss: 0.0675\n",
            "   Train: Loss=0.1289, Acc=0.975\n",
            "   Val:   Loss=0.1181, Acc=0.978 \n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.978\n",
            "Macro-F1 Score (Best Epoch): 0.9784\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2361    7    0    0    7    0    0    0    2    0    0    1    2    0\n",
            "     0    0    0    0    6    8    0    1    1    0    0    4    0    0\n",
            "     0]\n",
            " [   4 2380    0    1    0    0    0    0    2    0    3    0    0    0\n",
            "     0    0    0    1    3    0    4    0    0    0    0    0    0    0\n",
            "     2]\n",
            " [   0    0 2393    3    0    0    0    0    0    0    0    0    0    0\n",
            "     1    0    1    0    2    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    1 2387    2    2    0    0    0    0    0    0    0    0\n",
            "     4    1    0    0    0    0    2    0    0    0    0    0    0    0\n",
            "     1]\n",
            " [  20    7    0    0 2333    0    0    0   12    0    0    0    2    2\n",
            "     0    0    0    0   18    0    2    2    0    1    0    1    0    0\n",
            "     0]\n",
            " [   4    1    0    0    0 2384    0    0    1    2    0    1    0    0\n",
            "     0    1    0    0    0    2    0    3    1    0    0    0    0    0\n",
            "     0]\n",
            " [   3    3    0    0    3    0 2362    9    1    1    3    0    1    1\n",
            "     0    0    0    1    2    0    1    0    1    1    4    1    0    0\n",
            "     2]\n",
            " [   0    0    0    0    0    0    5 2393    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    1    0    0    1\n",
            "     0]\n",
            " [   1    2    0    0    8    0    0    0 2343    2    4    0    1    0\n",
            "     1    0    0    5   13    0    6    1    1    8    4    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    1    0    1    2    9 2374    0    0    0    2\n",
            "     0    0    0    4    5    0    0    0    0    0    1    0    1    0\n",
            "     0]\n",
            " [   0    0    0    1    1    0    3    1    4    1 2348    0    1    0\n",
            "     0    0    0   12    2    0    3   13   10    0    0    0    0    0\n",
            "     0]\n",
            " [   3    0    0    0    0    0    0    0    0    1    2 2382    0    0\n",
            "     0    0    0    0    0    6    2    0    0    1    1    2    0    0\n",
            "     0]\n",
            " [   1    6    0    0    0    0    0    0    0    0    0    0 2323   57\n",
            "     0    0    0    0   10    1    0    0    0    1    0    1    0    0\n",
            "     0]\n",
            " [   0    3    0    0    0    0    1    0    0    0    0    0   40 2347\n",
            "     0    0    0    0    4    0    0    0    0    0    0    0    4    0\n",
            "     1]\n",
            " [   0    0    0    0    1    0    0    0    0    1    0    0    0    1\n",
            "  2382    1    0    0    7    0    0    0    0    2    1    4    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2384    9    0    1    0    0    0    0    0    0    5    1    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    1    0    0    0    0    0    0\n",
            "     0   10 2386    0    0    0    0    0    0    0    0    0    3    0\n",
            "     0]\n",
            " [   1    1    0    0    0    0    1    0    9    2   11    0    0    1\n",
            "     1    0    0 2267    4    0   67    8    6   14    2    0    4    0\n",
            "     1]\n",
            " [   5    0    0    0   13    0    0    0    2    2    1    0    0    2\n",
            "     1    1    0    6 2294   13    5    0    2   50    1    1    1    0\n",
            "     0]\n",
            " [   6    0    1    0    2    0    0    0    2    0    0    5    0    0\n",
            "     1    3    0    0   14 2327    2    1    2   12   19    2    1    0\n",
            "     0]\n",
            " [   1    1    0    0    3    0    1    0    3    1    5    0    1    0\n",
            "     1    0    0   60    2    0 2259   27    8   21    1    2    0    0\n",
            "     3]\n",
            " [   0    0    0    2    1    1    0    0    7    0   24    1    0    0\n",
            "     0    0    0   12    2    1   40 2234   58   15    2    0    0    0\n",
            "     0]\n",
            " [   0    1    0    0    0    0    0    0    1    0    5    1    0    0\n",
            "     0    0    0    3    0    0    7   41 2339    1    1    0    0    0\n",
            "     0]\n",
            " [   1    2    0    0    3    0    0    0    6    3    0    3    0    6\n",
            "     1    0    0   10   51    9   40   13    2 2231    8    8    3    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    3    0    2    0    4    1    0    0\n",
            "     0    0    0    0    4   20    1    1    7    7 2342    8    0    0\n",
            "     0]\n",
            " [   1    0    2    0    2    0    0    0    1    2    0    2    0    0\n",
            "     0    0    0    0    9    9    1    5    0    2    3 2361    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    1    1    0\n",
            "     1    0    1    1    1    2    0    0    0    0    0    1 2391    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    1    0    0    0    0    1    0    0    1    0    0\n",
            "     0    1    0    5    1    0    0    0    0    1    2    0    0    0\n",
            "  2387]]\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.1\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Batch 0/272, Loss: 0.1425\n",
            "    Batch 100/272, Loss: 0.1386\n",
            "    Batch 200/272, Loss: 0.0773\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1047700668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#Append losses for plotting purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2572395175.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "head_loss_epoch_train = {}\n",
        "head_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_HEAD_ONLY = 3\n",
        "LR_HEAD = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = [64, 128, 256]\n",
        "\n",
        "for x in range(3):\n",
        "    #redefine model for each set of hyper paramaters\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    #Freeze Layers every Loop iteration\n",
        "    set_requires_grad(model, False)\n",
        "    set_requires_grad(model.fc, True)\n",
        "\n",
        "    #Create dictionary entry for losses by epoch across val and train\n",
        "    head_loss_epoch_train[x]=[]\n",
        "    head_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_HEAD[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_HEAD[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_HEAD[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_HEAD[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_HEAD_ONLY + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_HEAD_ONLY}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        head_loss_epoch_train[x].append(train_loss)\n",
        "        head_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5603562",
      "metadata": {
        "id": "f5603562"
      },
      "source": [
        "# TB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a92a97",
      "metadata": {
        "id": "e4a92a97"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 2\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TB_loss_epoch_train = {}\n",
        "TB_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TB = 3\n",
        "LR_TB = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "# Step 2: Unfreeze the classifier head and last layer\n",
        "print(\"\\n Unfreezing classifier head...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "set_requires_grad(model.layer4, True)\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TB_loss_epoch_train[x]=[]\n",
        "    TB_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_TB[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_TB[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_TB[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_TB[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_TB + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_TB}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        TB_loss_epoch_train[x].append(train_loss)\n",
        "        TB_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12982a6b",
      "metadata": {
        "id": "12982a6b"
      },
      "source": [
        "# Model TC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7132526",
      "metadata": {
        "id": "c7132526"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 3\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TC_loss_epoch_train = {}\n",
        "TC_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TC = 3\n",
        "LR_TC = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "# Step 2: Unfreeze the classifier head and last 2 layers\n",
        "print(\"\\n Unfreezing classifier head...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "set_requires_grad(model.layer4, True)\n",
        "set_requires_grad(model.layer3, True)\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TC_loss_epoch_train[x]=[]\n",
        "    TC_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_TC[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_TC[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_TC[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_TC[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_TC + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_TC}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        TC_loss_epoch_train[x].append(train_loss)\n",
        "        TC_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea79058",
      "metadata": {
        "id": "aea79058"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64ac2759",
      "metadata": {
        "id": "64ac2759"
      },
      "source": [
        "# Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13024128",
      "metadata": {
        "id": "13024128"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 4\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "full_loss_epoch_train = {}\n",
        "full_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_full = 3\n",
        "LR_full = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, True)\n",
        "\n",
        "# Step 2: None Frozen\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TC_loss_epoch_train[x]=[]\n",
        "    TC_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_full[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_full[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_full[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_full[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_full + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_full}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        full_loss_epoch_train[x].append(train_loss)\n",
        "        full_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}