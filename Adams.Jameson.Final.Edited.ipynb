{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b6277ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b6277ca",
        "outputId": "9a7f36dc-74be-4801-e90d-e3ac6505a8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard PyTorch + Torchvision stack\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Reproducibility (essential for research and debugging)\n",
        "import random\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Note: For complete reproducibility, you may also need:\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device (GPU if available)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8uD6Fg9ciL",
        "outputId": "f5e08cb6-15dc-4a89-dccc-c6f187110127"
      },
      "id": "KJ8uD6Fg9ciL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1c593ca7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c593ca7",
        "outputId": "80dea4bd-adde-427e-fde9-c17492e96bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 219MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Let's examine a fresh ResNet-18 pretrained on ImageNet\n",
        "res18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c363e3",
      "metadata": {
        "id": "88c363e3"
      },
      "source": [
        "## 3) Why Transfer Learning? The Power of Pretrained Features\n",
        "\n",
        "### The Transfer Learning Hypothesis\n",
        "\n",
        "Networks trained on large datasets (like ImageNet with 1.2M images, 1000 classes) learn hierarchical features:\n",
        "\n",
        "Early layers (conv1, layer1): Low-level features (edges, textures, colors)\n",
        "\n",
        "Middle layers (layer2, layer3): Mid-level features (shapes, parts, patterns)\n",
        "\n",
        "Deep layers (layer4): High-level, task-specific features (object parts)\n",
        "\n",
        "Final layer (fc): Class-specific decision boundaries\n",
        "\n",
        "Key Insight: Low and mid-level features are universal across vision tasks! We can reuse them and only adapt the high-level features to our new task.\n",
        "\n",
        "## Fine-Tuning Strategies\n",
        "\n",
        "### 1) Feature Extraction *(Freeze all, train head)*\n",
        "- **Pros:** Fastest; lowest overfitting risk  \n",
        "- **Use when:** Limited data; domain ≈ ImageNet  \n",
        "- **Unfrozen:** `fc` only\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Shallow Fine-Tuning *(Unfreeze layer4 + head)*\n",
        "- **Pros:** Adapts high-level features; still efficient  \n",
        "- **Use when:** Moderate data; somewhat different domain  \n",
        "- **Unfrozen:** `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Deep Fine-Tuning *(Unfreeze layer3 + layer4 + head)*\n",
        "- **Pros:** Greater adaptation capacity  \n",
        "- **Use when:** Sufficient data; noticeable domain shift  \n",
        "- **Unfrozen:** `layer3`, `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Fine-Tuning *(Unfreeze everything)*\n",
        "- **Pros:** Maximum flexibility  \n",
        "- **Cons:** Slowest; higher overfitting risk  \n",
        "- **Use when:** Large dataset; very different domain  \n",
        "- **Unfrozen:** all layers\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Tips\n",
        "- Prefer **smaller LR** for earlier layers (discriminative LRs).\n",
        "- Add regularization when unfreezing more (augmentations, weight decay, label smoothing).\n",
        "- Monitor validation; consider early stopping/checkpointing.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c1c800",
      "metadata": {
        "id": "56c1c800"
      },
      "source": [
        "## 4) Data Preprocessing: Why ImageNet Statistics?\n",
        "\n",
        "### Understanding ImageNet Normalization\n",
        "Pretrained networks expect inputs with specific statistics because they were trained on normalized ImageNet data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6c7c62b",
      "metadata": {
        "id": "c6c7c62b"
      },
      "outputs": [],
      "source": [
        "# ImageNet channel-wise statistics (computed over millions of images)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # Mean per channel (R, G, B)\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]  # Std dev per channel\n",
        "\n",
        "# Why these specific values?\n",
        "# - They center the data around 0 and scale to ~[-2, 2] range\n",
        "# - This matches the distribution the network was trained on\n",
        "# - Network weights are calibrated to these input scales"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Train_Data\" /content/\n",
        "!cp -r \"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\" /content/"
      ],
      "metadata": {
        "id": "AuBWbw-jGWuE"
      },
      "id": "AuBWbw-jGWuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b79974f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b79974f",
        "outputId": "0bcbd02f-ea95-459c-dc8f-15ae0946d7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            " Dataset: 83,996 train, 28 val\n",
            " Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space'] and test classes ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224          # Standard ImageNet size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Training transforms: Add variability to prevent overfitting\n",
        "train_tf = transforms.Compose([\n",
        "    # 1. Resize: ASL images need to be 224×224\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "\n",
        "    # 3. Convert to tensor: PIL Image → Tensor, scales to [0,1]\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # 4. Normalize: Match ImageNet statistics\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    # This does: output = (input - mean) / std\n",
        "])\n",
        "\n",
        "# Validation transforms: No augmentation (we want consistent evaluation)\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Load ASL dataset\n",
        "full_train_ds = datasets.ImageFolder(root=\"/content/Train_Data\",transform=train_tf)\n",
        "given_test_ds = datasets.ImageFolder(root=\"/content/Given_Test\",transform=val_tf)\n",
        "\n",
        "#full_train_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Train_Data\", transform=train_tf)\n",
        "#given_test_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\",transform=val_tf)\n",
        "\n",
        "print('here')\n",
        "#Need to split full train into validation set\n",
        "indices = np.arange(len( full_train_ds ) )\n",
        "labels = np.array(full_train_ds.targets ) # ASL labels\n",
        "train_idx , val_idx = train_test_split(indices , test_size =0.2 , stratify = labels , random_state =429)\n",
        "\n",
        "\n",
        "train_subset_ds = Subset(full_train_ds, train_idx)\n",
        "\n",
        "# Create the stratified Validation Subset\n",
        "val_subset_ds = Subset(full_train_ds, val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    given_test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "NUM_CLASSES = 28\n",
        "print(f' Dataset: {len(full_train_ds):,} train, {len(given_test_ds):,} val')\n",
        "print(f' Classes: {full_train_ds.classes} and test classes {given_test_ds.classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e90d56c0",
      "metadata": {
        "id": "e90d56c0"
      },
      "source": [
        "## 5) Model Setup: Adapting ResNet-18 for ASL Translation\n",
        "\n",
        "Replacing the Classification Head\n",
        "\n",
        "The pretrained ResNet-18 outputs 1000 classes (ImageNet), but we need 28 (ASL no del):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5bde5ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bde5ef",
        "outputId": "3809c295-309f-4e53-e594-4972325a35c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 1000 (ImageNet classes)\n",
            "\n",
            " New FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 28 (our classes)\n"
          ]
        }
      ],
      "source": [
        "# Start with ImageNet-pretrained weights\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Examine the original classifier\n",
        "print(\" Original FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (ImageNet classes)\")\n",
        "\n",
        "# Replace with our custom classifier\n",
        "# The in_features must match (512 for ResNet-18's final feature size)\n",
        "# The NUM_CLASSES will change for other datasets\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "print(\"\\n New FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (our classes)\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a03d7b",
      "metadata": {
        "id": "11a03d7b"
      },
      "source": [
        "## Understanding Parameter Names and Hierarchy\n",
        "To selectively freeze/unfreeze layers, we need to understand PyTorch's parameter naming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e0b8ca81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b8ca81",
        "outputId": "06a61f97-9335-4829-aa03-12901d9b29c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Structure (hierarchical view):\n",
            "├── conv1: Conv2d (9,408 params, 9,408 trainable)\n",
            "├── bn1: BatchNorm2d (128 params, 128 trainable)\n",
            "├── relu: ReLU (0 params, 0 trainable)\n",
            "├── maxpool: MaxPool2d (0 params, 0 trainable)\n",
            "├── layer1: Sequential (147,968 params, 147,968 trainable)\n",
            "│   ├── 0: BasicBlock (73,984 params, 73,984 trainable)\n",
            "│   ├── 1: BasicBlock (73,984 params, 73,984 trainable)\n",
            "├── layer2: Sequential (525,568 params, 525,568 trainable)\n",
            "│   ├── 0: BasicBlock (230,144 params, 230,144 trainable)\n",
            "│   ├── 1: BasicBlock (295,424 params, 295,424 trainable)\n",
            "├── layer3: Sequential (2,099,712 params, 2,099,712 trainable)\n",
            "│   ├── 0: BasicBlock (919,040 params, 919,040 trainable)\n",
            "│   ├── 1: BasicBlock (1,180,672 params, 1,180,672 trainable)\n",
            "├── layer4: Sequential (8,393,728 params, 8,393,728 trainable)\n",
            "│   ├── 0: BasicBlock (3,673,088 params, 3,673,088 trainable)\n",
            "│   ├── 1: BasicBlock (4,720,640 params, 4,720,640 trainable)\n",
            "├── avgpool: AdaptiveAvgPool2d (0 params, 0 trainable)\n",
            "├── fc: Linear (14,364 params, 14,364 trainable)\n"
          ]
        }
      ],
      "source": [
        "def explore_model_structure(model, max_depth=2):\n",
        "    \"\"\"Visualize the model's hierarchical structure\"\"\"\n",
        "\n",
        "    print(\"\\n Model Structure (hierarchical view):\")\n",
        "\n",
        "    def print_module(module, prefix=\"\", depth=0):\n",
        "        if depth >= max_depth:\n",
        "            return\n",
        "        for name, child in module.named_children():\n",
        "            param_count = sum(p.numel() for p in child.parameters())\n",
        "            trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
        "            print(f\"{prefix}├── {name}: {child.__class__.__name__} \"\n",
        "                  f\"({param_count:,} params, {trainable:,} trainable)\")\n",
        "            if depth < max_depth - 1:\n",
        "                print_module(child, prefix + \"│   \", depth + 1)\n",
        "\n",
        "    print_module(model)\n",
        "\n",
        "# Explore structure\n",
        "explore_model_structure(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c35761",
      "metadata": {
        "id": "d2c35761"
      },
      "source": [
        "## 6) Freezing and Unfreezing: The Core Mechanism\n",
        "### How Freezing Works\n",
        "When we \"freeze\" a layer, we set requires_grad=False on its parameters:\n",
        "\n",
        "Frozen parameters: No gradients computed, no updates during backprop\n",
        "\n",
        "Unfrozen parameters: Gradients computed, weights updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9c5647d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c5647d8",
        "outputId": "470c7531-a42d-4228-ffc3-60cedffbdb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Freezing entire model...\n",
            "  ResNet: 11,190,876 parameters FROZEN\n",
            "\n",
            " Unfreezing only the FC layer...\n",
            "  Linear: 14,364 parameters UNFROZEN (trainable)\n",
            "\n",
            " Trainable: 14,364 / 11,190,876 parameters (0.128355%)\n"
          ]
        }
      ],
      "source": [
        "def set_requires_grad(module: nn.Module, requires_grad: bool):\n",
        "    \"\"\"\n",
        "    Recursively set requires_grad for all parameters in a module.\n",
        "\n",
        "    Args:\n",
        "        module: PyTorch module (layer, block, or entire model)\n",
        "        requires_grad: True to unfreeze (train), False to freeze\n",
        "    \"\"\"\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "    # Print status\n",
        "    param_count = sum(p.numel() for p in module.parameters())\n",
        "    status = \"UNFROZEN (trainable)\" if requires_grad else \"FROZEN\"\n",
        "    print(f\"  {module.__class__.__name__}: {param_count:,} parameters {status}\")\n",
        "\n",
        "# Example: Freeze entire model, then selectively unfreeze\n",
        "print(\" Freezing entire model...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "print(\"\\n Unfreezing only the FC layer...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "\n",
        "# Verify what's trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n Trainable: {trainable_params:,} / {total_params:,} parameters \"\n",
        "      f\"({100*trainable_params/total_params:.6f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ee588b",
      "metadata": {
        "id": "36ee588b"
      },
      "source": [
        "## 7) Training Infrastructure\n",
        "\n",
        "Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "63addda7",
      "metadata": {
        "id": "63addda7"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout, batch norm training mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        # Optional: Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"    Batch {batch_idx}/{len(loader)}, \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "        print('Here 1')\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()  # Decorator disables gradient computation\n",
        "def evaluate(model, loader):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation/test set.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout, batch norm eval mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "    all_preds = [] # <-- Collects all predicted labels\n",
        "    all_targets = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass only (no backward)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        print('here2')\n",
        "        all_preds.extend(predictions.cpu().tolist())\n",
        "        # FIX HERE: Use 'labels' instead of 'targets'\n",
        "        all_targets.extend(labels.cpu().tolist())\n",
        "\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy, all_targets, all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d5930662",
      "metadata": {
        "id": "d5930662"
      },
      "outputs": [],
      "source": [
        "#Define dictionary that is going to hold the loss accross epochs for each model for comparison purposes\n",
        "total_loss_by_epoch_model = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142d945e",
      "metadata": {
        "id": "142d945e"
      },
      "source": [
        "## 8) Phase 1.1: Head-Only Fine-Tuning (Feature Extraction)\n",
        "Strategy: Use ResNet as a Fixed Feature Extractor\n",
        "\n",
        "In this phase, we:\n",
        "\n",
        "1. Freeze all convolutional layers (keep ImageNet features)\n",
        "\n",
        "2. Train only the new classifier head (learn new class boundaries)\n",
        "\n",
        "3. Use higher learning rate (since we're training from scratch)\n",
        "\n",
        "This is the safest approach with limited data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "78d0ce38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78d0ce38",
        "outputId": "50977834-9ec0-4722-ce83-e2adfeacc9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,190,876 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,364 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 14,364\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/1050, Loss: 3.6351\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n",
            "Here 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-431258682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-603554811.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Move data to device (GPU/CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "head_loss_epoch_train = {}\n",
        "head_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_HEAD_ONLY = 3\n",
        "LR_HEAD = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "# Step 2: Unfreeze only the classifier head\n",
        "print(\"\\n Unfreezing classifier head...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [64, 128, 256]\n",
        "\n",
        "for x in range(3):\n",
        "    head_loss_epoch_train[x]=[]\n",
        "    head_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_HEAD[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_HEAD[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_HEAD[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_HEAD[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_HEAD_ONLY + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_HEAD_ONLY}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        head_loss_epoch_train[x].append(train_loss)\n",
        "        head_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5603562",
      "metadata": {
        "id": "f5603562"
      },
      "source": [
        "# TB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a92a97",
      "metadata": {
        "id": "e4a92a97"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 2\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TB_loss_epoch_train = {}\n",
        "TB_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TB = 3\n",
        "LR_TB = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "# Step 2: Unfreeze the classifier head and last layer\n",
        "print(\"\\n Unfreezing classifier head...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "set_requires_grad(model.layer4, True)\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TB_loss_epoch_train[x]=[]\n",
        "    TB_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_TB[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_TB[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_TB[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_TB[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_TB + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_TB}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        TB_loss_epoch_train[x].append(train_loss)\n",
        "        TB_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12982a6b",
      "metadata": {
        "id": "12982a6b"
      },
      "source": [
        "# Model TC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7132526",
      "metadata": {
        "id": "c7132526"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 3\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TC_loss_epoch_train = {}\n",
        "TC_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TC = 3\n",
        "LR_TC = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "# Step 2: Unfreeze the classifier head and last 2 layers\n",
        "print(\"\\n Unfreezing classifier head...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "set_requires_grad(model.layer4, True)\n",
        "set_requires_grad(model.layer3, True)\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TC_loss_epoch_train[x]=[]\n",
        "    TC_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_TC[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_TC[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_TC[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_TC[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_TC + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_TC}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        TC_loss_epoch_train[x].append(train_loss)\n",
        "        TC_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea79058",
      "metadata": {
        "id": "aea79058"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64ac2759",
      "metadata": {
        "id": "64ac2759"
      },
      "source": [
        "# Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13024128",
      "metadata": {
        "id": "13024128"
      },
      "outputs": [],
      "source": [
        "#Redefine model for phase 4\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "full_loss_epoch_train = {}\n",
        "full_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_full = 3\n",
        "LR_full = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, True)\n",
        "\n",
        "# Step 2: None Frozen\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "    TC_loss_epoch_train[x]=[]\n",
        "    TC_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_full[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_full[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_full[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_full[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_full + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_full}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        full_loss_epoch_train[x].append(train_loss)\n",
        "        full_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}