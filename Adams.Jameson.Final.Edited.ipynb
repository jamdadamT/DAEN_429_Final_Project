{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b6277ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b6277ca",
        "outputId": "1c70a1ce-37df-435a-cd6a-e960b20baaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard PyTorch + Torchvision stack\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Reproducibility (essential for research and debugging)\n",
        "import random\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Note: For complete reproducibility, you may also need:\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device (GPU if available)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8uD6Fg9ciL",
        "outputId": "a188c319-a8a2-466f-c55e-ba8d422d9782"
      },
      "id": "KJ8uD6Fg9ciL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c593ca7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c593ca7",
        "outputId": "269dea50-446f-467a-d2ba-37de9cd0eaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 98.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Let's examine a fresh ResNet-18 pretrained on ImageNet\n",
        "res18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c363e3",
      "metadata": {
        "id": "88c363e3"
      },
      "source": [
        "## 3) Why Transfer Learning? The Power of Pretrained Features\n",
        "\n",
        "### The Transfer Learning Hypothesis\n",
        "\n",
        "Networks trained on large datasets (like ImageNet with 1.2M images, 1000 classes) learn hierarchical features:\n",
        "\n",
        "Early layers (conv1, layer1): Low-level features (edges, textures, colors)\n",
        "\n",
        "Middle layers (layer2, layer3): Mid-level features (shapes, parts, patterns)\n",
        "\n",
        "Deep layers (layer4): High-level, task-specific features (object parts)\n",
        "\n",
        "Final layer (fc): Class-specific decision boundaries\n",
        "\n",
        "Key Insight: Low and mid-level features are universal across vision tasks! We can reuse them and only adapt the high-level features to our new task.\n",
        "\n",
        "## Fine-Tuning Strategies\n",
        "\n",
        "### 1) Feature Extraction *(Freeze all, train head)*\n",
        "- **Pros:** Fastest; lowest overfitting risk  \n",
        "- **Use when:** Limited data; domain ≈ ImageNet  \n",
        "- **Unfrozen:** `fc` only\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Shallow Fine-Tuning *(Unfreeze layer4 + head)*\n",
        "- **Pros:** Adapts high-level features; still efficient  \n",
        "- **Use when:** Moderate data; somewhat different domain  \n",
        "- **Unfrozen:** `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Deep Fine-Tuning *(Unfreeze layer3 + layer4 + head)*\n",
        "- **Pros:** Greater adaptation capacity  \n",
        "- **Use when:** Sufficient data; noticeable domain shift  \n",
        "- **Unfrozen:** `layer3`, `layer4`, `fc`\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Fine-Tuning *(Unfreeze everything)*\n",
        "- **Pros:** Maximum flexibility  \n",
        "- **Cons:** Slowest; higher overfitting risk  \n",
        "- **Use when:** Large dataset; very different domain  \n",
        "- **Unfrozen:** all layers\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Tips\n",
        "- Prefer **smaller LR** for earlier layers (discriminative LRs).\n",
        "- Add regularization when unfreezing more (augmentations, weight decay, label smoothing).\n",
        "- Monitor validation; consider early stopping/checkpointing.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c1c800",
      "metadata": {
        "id": "56c1c800"
      },
      "source": [
        "## 4) Data Preprocessing: Why ImageNet Statistics?\n",
        "\n",
        "### Understanding ImageNet Normalization\n",
        "Pretrained networks expect inputs with specific statistics because they were trained on normalized ImageNet data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6c7c62b",
      "metadata": {
        "id": "c6c7c62b"
      },
      "outputs": [],
      "source": [
        "# ImageNet channel-wise statistics (computed over millions of images)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # Mean per channel (R, G, B)\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]  # Std dev per channel\n",
        "\n",
        "# Why these specific values?\n",
        "# - They center the data around 0 and scale to ~[-2, 2] range\n",
        "# - This matches the distribution the network was trained on\n",
        "# - Network weights are calibrated to these input scales"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRGFc2kLOCov",
        "outputId": "ba23b259-2585-422c-a653-9b6d566d7a6e"
      },
      "id": "HRGFc2kLOCov",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'asl-alphabet' dataset.\n",
            "Path to dataset files: /kaggle/input/asl-alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "IBl1Lx9rMSec",
        "outputId": "b8825f5e-b3f2-434f-b5ed-b271bf3228f7"
      },
      "id": "IBl1Lx9rMSec",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fe19794c-9767-4801-af4b-e96e4f0c0d44\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fe19794c-9767-4801-af4b-e96e4f0c0d44\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1613494533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    174\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Train_Data.zip -d /content/"
      ],
      "metadata": {
        "id": "gDASgn89HPk0"
      },
      "id": "gDASgn89HPk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\" /content/"
      ],
      "metadata": {
        "id": "cCeYJpjaHT8j"
      },
      "id": "cCeYJpjaHT8j",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b79974f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b79974f",
        "outputId": "7effbff3-a818-4bc2-a1b3-e7b12ff900c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "in between\n",
            "here\n",
            " Dataset: 87,000 train, 28 val\n",
            " Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] and test classes ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224          # Standard ImageNet size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Training transforms: Add variability to prevent overfitting\n",
        "train_tf = transforms.Compose([\n",
        "    # 1. Resize: ASL images need to be 224×224\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "\n",
        "    # 3. Convert to tensor: PIL Image → Tensor, scales to [0,1]\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # 4. Normalize: Match ImageNet statistics\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    # This does: output = (input - mean) / std\n",
        "])\n",
        "\n",
        "# Validation transforms: No augmentation (we want consistent evaluation)\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "print('here')\n",
        "# Load ASL dataset\n",
        "full_train_ds = datasets.ImageFolder(root=\"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\",transform=train_tf)\n",
        "print('in between')\n",
        "given_test_ds = datasets.ImageFolder(root=\"/content/Given_Test\",transform=val_tf)\n",
        "\n",
        "#full_train_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Train_Data\", transform=train_tf)\n",
        "#given_test_ds = datasets.ImageFolder(root=\"/content/drive/MyDrive/Adams.Daen429.Final.Proj/Given_Test\",transform=val_tf)\n",
        "\n",
        "print('here')\n",
        "#Need to split full train into validation set\n",
        "indices = np.arange(len( full_train_ds ) )\n",
        "labels = np.array(full_train_ds.targets ) # ASL labels\n",
        "train_idx , val_idx = train_test_split(indices , test_size =0.2 , stratify = labels , random_state =429)\n",
        "\n",
        "\n",
        "train_subset_ds = Subset(full_train_ds, train_idx)\n",
        "\n",
        "# Create the stratified Validation Subset\n",
        "val_subset_ds = Subset(full_train_ds, val_idx)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    given_test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "NUM_CLASSES = 29\n",
        "print(f' Dataset: {len(full_train_ds):,} train, {len(given_test_ds):,} val')\n",
        "print(f' Classes: {full_train_ds.classes} and test classes {given_test_ds.classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e90d56c0",
      "metadata": {
        "id": "e90d56c0"
      },
      "source": [
        "## 5) Model Setup: Adapting ResNet-18 for ASL Translation\n",
        "\n",
        "Replacing the Classification Head\n",
        "\n",
        "The pretrained ResNet-18 outputs 1000 classes (ImageNet), but we need 28 (ASL no del):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b5bde5ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bde5ef",
        "outputId": "349c5be4-c957-4984-ba5c-8cd7001d8f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 1000 (ImageNet classes)\n",
            "\n",
            " New FC layer:\n",
            "  Input features: 512\n",
            "  Output features: 29 (our classes)\n"
          ]
        }
      ],
      "source": [
        "# Start with ImageNet-pretrained weights\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Examine the original classifier\n",
        "print(\" Original FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (ImageNet classes)\")\n",
        "\n",
        "# Replace with our custom classifier\n",
        "# The in_features must match (512 for ResNet-18's final feature size)\n",
        "# The NUM_CLASSES will change for other datasets\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "print(\"\\n New FC layer:\")\n",
        "print(f\"  Input features: {model.fc.in_features}\")\n",
        "print(f\"  Output features: {model.fc.out_features} (our classes)\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a03d7b",
      "metadata": {
        "id": "11a03d7b"
      },
      "source": [
        "## Understanding Parameter Names and Hierarchy\n",
        "To selectively freeze/unfreeze layers, we need to understand PyTorch's parameter naming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0b8ca81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b8ca81",
        "outputId": "8c78121d-ff75-485e-ce51-6393eceddfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Structure (hierarchical view):\n",
            "├── conv1: Conv2d (9,408 params, 9,408 trainable)\n",
            "├── bn1: BatchNorm2d (128 params, 128 trainable)\n",
            "├── relu: ReLU (0 params, 0 trainable)\n",
            "├── maxpool: MaxPool2d (0 params, 0 trainable)\n",
            "├── layer1: Sequential (147,968 params, 147,968 trainable)\n",
            "│   ├── 0: BasicBlock (73,984 params, 73,984 trainable)\n",
            "│   ├── 1: BasicBlock (73,984 params, 73,984 trainable)\n",
            "├── layer2: Sequential (525,568 params, 525,568 trainable)\n",
            "│   ├── 0: BasicBlock (230,144 params, 230,144 trainable)\n",
            "│   ├── 1: BasicBlock (295,424 params, 295,424 trainable)\n",
            "├── layer3: Sequential (2,099,712 params, 2,099,712 trainable)\n",
            "│   ├── 0: BasicBlock (919,040 params, 919,040 trainable)\n",
            "│   ├── 1: BasicBlock (1,180,672 params, 1,180,672 trainable)\n",
            "├── layer4: Sequential (8,393,728 params, 8,393,728 trainable)\n",
            "│   ├── 0: BasicBlock (3,673,088 params, 3,673,088 trainable)\n",
            "│   ├── 1: BasicBlock (4,720,640 params, 4,720,640 trainable)\n",
            "├── avgpool: AdaptiveAvgPool2d (0 params, 0 trainable)\n",
            "├── fc: Linear (14,877 params, 14,877 trainable)\n"
          ]
        }
      ],
      "source": [
        "def explore_model_structure(model, max_depth=2):\n",
        "    \"\"\"Visualize the model's hierarchical structure\"\"\"\n",
        "\n",
        "    print(\"\\n Model Structure (hierarchical view):\")\n",
        "\n",
        "    def print_module(module, prefix=\"\", depth=0):\n",
        "        if depth >= max_depth:\n",
        "            return\n",
        "        for name, child in module.named_children():\n",
        "            param_count = sum(p.numel() for p in child.parameters())\n",
        "            trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
        "            print(f\"{prefix}├── {name}: {child.__class__.__name__} \"\n",
        "                  f\"({param_count:,} params, {trainable:,} trainable)\")\n",
        "            if depth < max_depth - 1:\n",
        "                print_module(child, prefix + \"│   \", depth + 1)\n",
        "\n",
        "    print_module(model)\n",
        "\n",
        "# Explore structure\n",
        "explore_model_structure(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c35761",
      "metadata": {
        "id": "d2c35761"
      },
      "source": [
        "## 6) Freezing and Unfreezing: The Core Mechanism\n",
        "### How Freezing Works\n",
        "When we \"freeze\" a layer, we set requires_grad=False on its parameters:\n",
        "\n",
        "Frozen parameters: No gradients computed, no updates during backprop\n",
        "\n",
        "Unfrozen parameters: Gradients computed, weights updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9c5647d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c5647d8",
        "outputId": "e6c69cb5-396f-4357-8fba-a5ad974156f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Freezing entire model...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing only the FC layer...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Trainable: 14,877 / 11,191,389 parameters (0.132933%)\n"
          ]
        }
      ],
      "source": [
        "def set_requires_grad(module: nn.Module, requires_grad: bool):\n",
        "    \"\"\"\n",
        "    Recursively set requires_grad for all parameters in a module.\n",
        "\n",
        "    Args:\n",
        "        module: PyTorch module (layer, block, or entire model)\n",
        "        requires_grad: True to unfreeze (train), False to freeze\n",
        "    \"\"\"\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "    # Print status\n",
        "    param_count = sum(p.numel() for p in module.parameters())\n",
        "    status = \"UNFROZEN (trainable)\" if requires_grad else \"FROZEN\"\n",
        "    print(f\"  {module.__class__.__name__}: {param_count:,} parameters {status}\")\n",
        "\n",
        "# Example: Freeze entire model, then selectively unfreeze\n",
        "print(\" Freezing entire model...\")\n",
        "set_requires_grad(model, False)\n",
        "\n",
        "print(\"\\n Unfreezing only the FC layer...\")\n",
        "set_requires_grad(model.fc, True)\n",
        "\n",
        "# Verify what's trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n Trainable: {trainable_params:,} / {total_params:,} parameters \"\n",
        "      f\"({100*trainable_params/total_params:.6f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ee588b",
      "metadata": {
        "id": "36ee588b"
      },
      "source": [
        "## 7) Training Infrastructure\n",
        "\n",
        "Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "63addda7",
      "metadata": {
        "id": "63addda7"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout, batch norm training mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        # Optional: Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"    Batch {batch_idx}/{len(loader)}, \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "        #print('Here 1')\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()  # Decorator disables gradient computation\n",
        "def evaluate(model, loader):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation/test set.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    print('Reached Evaluate')\n",
        "    model.eval()  # Disable dropout, batch norm eval mode\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    running_loss = 0.0\n",
        "    all_preds = [] # <-- Collects all predicted labels\n",
        "    all_targets = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass only (no backward)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "\n",
        "        all_preds.extend(predictions.cpu().tolist())\n",
        "        # FIX HERE: Use 'labels' instead of 'targets'\n",
        "        all_targets.extend(labels.cpu().tolist())\n",
        "\n",
        "\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy, all_targets, all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d5930662",
      "metadata": {
        "id": "d5930662"
      },
      "outputs": [],
      "source": [
        "#Define dictionary that is going to hold the loss accross epochs for each model for comparison purposes\n",
        "total_loss_by_epoch_model = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142d945e",
      "metadata": {
        "id": "142d945e"
      },
      "source": [
        "## 8) Phase 1.1: Head-Only Fine-Tuning (Feature Extraction)\n",
        "Strategy: Use ResNet as a Fixed Feature Extractor\n",
        "\n",
        "In this phase, we:\n",
        "\n",
        "1. Freeze all convolutional layers (keep ImageNet features)\n",
        "\n",
        "2. Train only the new classifier head (learn new class boundaries)\n",
        "\n",
        "3. Use higher learning rate (since we're training from scratch)\n",
        "\n",
        "This is the safest approach with limited data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "78d0ce38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78d0ce38",
        "outputId": "72b79bf4-4caa-45c1-c86f-90ee339337a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/1088, Loss: 3.6567\n",
            "    Batch 100/1088, Loss: 1.6072\n",
            "    Batch 200/1088, Loss: 0.9691\n",
            "    Batch 300/1088, Loss: 0.8305\n",
            "    Batch 400/1088, Loss: 0.6775\n",
            "    Batch 500/1088, Loss: 0.6371\n",
            "    Batch 600/1088, Loss: 0.4267\n",
            "    Batch 700/1088, Loss: 0.5452\n",
            "    Batch 800/1088, Loss: 0.4572\n",
            "    Batch 900/1088, Loss: 0.3114\n",
            "    Batch 1000/1088, Loss: 0.3281\n",
            "   Train: Loss=0.8353, Acc=0.835\n",
            "   Val:   Loss=0.3267, Acc=0.938  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/1088, Loss: 0.3089\n",
            "    Batch 100/1088, Loss: 0.3100\n",
            "    Batch 200/1088, Loss: 0.3448\n",
            "    Batch 300/1088, Loss: 0.2943\n",
            "    Batch 400/1088, Loss: 0.3004\n",
            "    Batch 500/1088, Loss: 0.2615\n",
            "    Batch 600/1088, Loss: 0.3004\n",
            "    Batch 700/1088, Loss: 0.2225\n",
            "    Batch 800/1088, Loss: 0.2532\n",
            "    Batch 900/1088, Loss: 0.2003\n",
            "    Batch 1000/1088, Loss: 0.3412\n",
            "   Train: Loss=0.2735, Acc=0.943\n",
            "   Val:   Loss=0.1886, Acc=0.963  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/1088, Loss: 0.2188\n",
            "    Batch 100/1088, Loss: 0.1566\n",
            "    Batch 200/1088, Loss: 0.2127\n",
            "    Batch 300/1088, Loss: 0.1714\n",
            "    Batch 400/1088, Loss: 0.2698\n",
            "    Batch 500/1088, Loss: 0.2044\n",
            "    Batch 600/1088, Loss: 0.1891\n",
            "    Batch 700/1088, Loss: 0.1257\n",
            "    Batch 800/1088, Loss: 0.2317\n",
            "    Batch 900/1088, Loss: 0.1471\n",
            "    Batch 1000/1088, Loss: 0.1987\n",
            "   Train: Loss=0.1872, Acc=0.958\n",
            "   Val:   Loss=0.1333, Acc=0.972  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.972\n",
            "Macro-F1 Score (Best Epoch): 0.9721\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2376    2    0    0    2    0    0    0    1    0    2    1    3    0\n",
            "     0    0    0    0    3    6    0    0    2    0    0    2    0    0\n",
            "     0]\n",
            " [  12 2375    0    0    0    0    0    0    1    0    6    0    1    0\n",
            "     1    0    0    0    0    0    2    1    0    0    0    0    0    0\n",
            "     1]\n",
            " [   0    0 2392    2    0    0    0    0    0    0    0    0    0    1\n",
            "     4    0    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    2 2373    1    1    0    0    1    0    0    0    0    0\n",
            "    19    1    0    0    0    0    0    0    0    0    0    0    1    0\n",
            "     1]\n",
            " [  36    8    0    0 2323    0    0    0    7    0    0    0    4    2\n",
            "     2    0    0    0   11    0    1    2    0    3    0    1    0    0\n",
            "     0]\n",
            " [   4    2    0    1    1 2371    0    0    1    2    2    2    0    0\n",
            "     1    1    0    0    0    2    0    4    5    0    1    0    0    0\n",
            "     0]\n",
            " [   3    3    0    0    3    0 2367    6    1    0    4    0    1    0\n",
            "     0    0    0    0    0    0    0    0    1    1    5    1    1    0\n",
            "     3]\n",
            " [   0    0    0    0    0    0   19 2375    0    1    0    0    0    0\n",
            "     0    0    1    0    0    0    0    0    0    0    1    0    1    2\n",
            "     0]\n",
            " [   2    3    1    3   15    0    1    0 2302    5   20    2    1    0\n",
            "     8    0    0    2    5    0    4    1    1   19    5    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    5    1    8 2372    5    0    0    1\n",
            "     0    0    0    1    0    0    0    0    2    0    1    0    3    0\n",
            "     1]\n",
            " [   0    0    0    2    0    0    6    0    1    0 2349    0    1    0\n",
            "     0    0    0    4    1    0    1   14   20    1    0    0    0    0\n",
            "     0]\n",
            " [   4    0    1    0    0    0    0    0    0    1    2 2385    0    0\n",
            "     0    0    0    0    0    3    1    0    0    1    1    1    0    0\n",
            "     0]\n",
            " [   4    3    0    1    0    0    0    0    0    0    0    0 2360   27\n",
            "     0    0    0    0    1    0    1    0    0    2    0    1    0    0\n",
            "     0]\n",
            " [   0    4    0    0    0    0    1    0    0    0    0    0  101 2282\n",
            "     1    0    0    0    1    0    0    0    2    0    0    1    6    0\n",
            "     1]\n",
            " [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "  2390    0    0    0    1    0    0    0    0    3    0    4    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2383    6    0    0    0    0    0    0    0    0    6    5    0\n",
            "     0]\n",
            " [   0    0    1    0    0    0    0    1    0    0    0    0    0    0\n",
            "     1    8 2380    0    0    0    0    0    0    0    0    0    9    0\n",
            "     0]\n",
            " [   1    2    0    0    0    0    2    0    5    3   20    0    0    0\n",
            "     4    0    0 2225    0    0   72   18   16   23    2    0    5    0\n",
            "     2]\n",
            " [  17    1    0    0   18    0    6    0    1    3    3    1    4    4\n",
            "    18    4    0    7 2092   29   15    2    3  155    5    6    5    0\n",
            "     1]\n",
            " [  15    0    2    1    2    0    0    0    2    0    0   11    0    0\n",
            "     6    3    0    0    7 2275    2    1    3   31   35    1    3    0\n",
            "     0]\n",
            " [   2    3    0    0    5    0    3    0    5    1   11    3    4    0\n",
            "     6    0    0   71    0    0 2170   55   11   36    4    6    0    0\n",
            "     4]\n",
            " [   0    0    0    2    1    1    1    0    4    0   26    1    0    0\n",
            "     0    0    0    9    0    1   25 2252   57   18    1    0    0    0\n",
            "     1]\n",
            " [   0    0    0    0    0    0    0    0    0    0   10    1    0    0\n",
            "     0    0    0    2    0    0    7   58 2320    1    1    0    0    0\n",
            "     0]\n",
            " [   3    2    0    0    3    0    0    0    0    2    2    3    0    6\n",
            "     4    0    0    8   10    8   29   21    1 2277   10    7    3    0\n",
            "     1]\n",
            " [   0    0    0    0    0    0    7    0    1    0    4    1    0    0\n",
            "     0    0    0    0    1    9    1    2    9    7 2351    7    0    0\n",
            "     0]\n",
            " [   1    0    4    0    2    0    0    0    0    2    0    2    0    0\n",
            "     0    0    0    0    1    8    1    4    1    5    4 2365    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    1    0\n",
            "     1    0    1    1    0    0    0    0    0    0    0    3 2393    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    1    0    0    0    0    1    1    0    1    0    0\n",
            "     0    0    0    4    0    0    0    0    0    1    5    0    2    0\n",
            "  2384]]\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.01\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/544, Loss: 3.6042\n",
            "    Batch 100/544, Loss: 2.6373\n",
            "    Batch 200/544, Loss: 2.2249\n",
            "    Batch 300/544, Loss: 1.8440\n",
            "    Batch 400/544, Loss: 1.5378\n",
            "    Batch 500/544, Loss: 1.3824\n",
            "   Train: Loss=2.0909, Acc=0.610\n",
            "   Val:   Loss=1.3305, Acc=0.828  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/544, Loss: 1.2949\n",
            "    Batch 100/544, Loss: 1.2094\n",
            "    Batch 200/544, Loss: 1.1263\n",
            "    Batch 300/544, Loss: 1.0391\n",
            "    Batch 400/544, Loss: 0.8776\n",
            "    Batch 500/544, Loss: 0.9832\n",
            "   Train: Loss=1.0848, Acc=0.852\n",
            "   Val:   Loss=0.8731, Acc=0.881  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/544, Loss: 0.9141\n",
            "    Batch 100/544, Loss: 0.9241\n",
            "    Batch 200/544, Loss: 0.7871\n",
            "    Batch 300/544, Loss: 0.8279\n",
            "    Batch 400/544, Loss: 0.7080\n",
            "    Batch 500/544, Loss: 0.6869\n",
            "   Train: Loss=0.7846, Acc=0.886\n",
            "   Val:   Loss=0.6750, Acc=0.901  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.901\n",
            "Macro-F1 Score (Best Epoch): 0.9007\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2186   12    1    0   47    1    0    2    2    1    1   11   11    8\n",
            "     1    0    0    3   34   52    0    1    1    1    3   12    2    3\n",
            "     4]\n",
            " [  36 2229    1    8   25    5    1    3   13    0   16    4   16    0\n",
            "     1    0    0   14    8    1    9    1    6    1    0    0    2    0\n",
            "     0]\n",
            " [   0    7 2321    8    0    1    1    0    1    0    0    1    0    5\n",
            "    21   20    3    1    5    0    0    0    0    0    0    3    2    0\n",
            "     0]\n",
            " [   0    1   13 2282    3    6    3    0    5    0    0    1    0    0\n",
            "    49    7    6    2    2    1    3    3    0    0    0    1    0    0\n",
            "    12]\n",
            " [  60   47    0    5 2175    4    3    1   15    1    1    1    4    3\n",
            "     2    0    0    0   55    4    7    2    1    6    0    2    0    0\n",
            "     1]\n",
            " [   7   11    0    1    9 2276    2    0    6    5   14    3    0    0\n",
            "     2    1    0    1    3    3    0   13   24    7    9    1    0    0\n",
            "     2]\n",
            " [  12    9    2    0    4    8 2127  108    1   17   11    9    4    1\n",
            "     0    0    0    4    7    0    3    1    3    1   56    3    2    2\n",
            "     5]\n",
            " [   0    1    1    0    0    0   22 2330    1    4    0    0    0    1\n",
            "     0    0    0    0    0    0    5    0    1    0    6    0   10   17\n",
            "     1]\n",
            " [   6    8    2    4   55    0    4    0 2105   10   49   13    0    0\n",
            "     3    0    0   17   47    0   26    6    5   10   16    4    0    1\n",
            "     9]\n",
            " [   0    0    0    7    9    1   17    6    6 2203   12   16    0    1\n",
            "     0    0    0   19   16    0   15    9    4    0   19    7   23    0\n",
            "    10]\n",
            " [   2   10    4    0    0    1    6    3   17    1 2156    3    2    2\n",
            "     0    0    0   54    9    2   21   50   40    3    2    1    2    0\n",
            "     9]\n",
            " [  14    1    3    0    0    6    0    0    9    0    2 2283    0    0\n",
            "     0    0    0   25    0   10    2    3    0    6   14   20    0    0\n",
            "     2]\n",
            " [  13   14    0    0    0    3    2    1    0    0    0    1 2153  139\n",
            "     3    1    0    8   45    4    5    0    0    5    0    2    0    0\n",
            "     1]\n",
            " [   5   10    5    0    0    0    0    3    0    0    0    1  211 2091\n",
            "     1    0    1    0   29    1    3    0    0    0    2    7   24    0\n",
            "     6]\n",
            " [   0    0    1    3    2    0    0    0    1    2    0    0    3    5\n",
            "  2313    2    2    6   29    2    3    2    0    1    2   20    0    1\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    4    0    0\n",
            "     5 2296   49    0    0    2    0    0    0    0    0   13   24    0\n",
            "     7]\n",
            " [   0    0    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0   67 2303    0    0    0    0    0    0    0    0    4   23    0\n",
            "     0]\n",
            " [   0    6    1    0    0    0    2    1   24    3   15    0    7    7\n",
            "     1    0    1 2074    7    6  130   21   36   27    5    4   12    0\n",
            "    10]\n",
            " [  23    2    0    0  103    0    0    0    8    3    7    0    3    3\n",
            "    15    4    1   12 1955   71   30    9    6   95    9   26   11    3\n",
            "     1]\n",
            " [  13    8   17    0   12    2    0    0   12    0    0   29    3    8\n",
            "     9    7    0    0   66 2079    4    2   12   19   61   26    6    0\n",
            "     5]\n",
            " [  21   10    0    0   22    0    1    6   75    0   19    2   14    1\n",
            "    11    0    0  201   10    8 1823   83   13   27    5   23    1    0\n",
            "    24]\n",
            " [   0    0    0    1   13    1    1    0   51    0   81   13    1    1\n",
            "     0    1    0  124    6    6   71 1730  208   42   20   15    0    0\n",
            "    14]\n",
            " [   0    0    0    0    1    0   10    0    1    0   21    1    0    0\n",
            "     1    0    0   19    0    2   16  104 2206    0    6    9    0    0\n",
            "     3]\n",
            " [  16    2    3    1   19    1    2    0   39    2    9   10    8    5\n",
            "     0    1    0   28  143  168  145   31    4 1676   29   30    8    0\n",
            "    20]\n",
            " [   2    0    0    0    0   10   15    1    5    2    5   12    2    5\n",
            "     0    2    1    3    8  161    1    3   32    0 2098   23    3    0\n",
            "     6]\n",
            " [   2    0   10    0    1    0    0    0   10    9    0    4    0    2\n",
            "     0    0    1   10   24   40    2    1    0    2   21 2255    1    0\n",
            "     5]\n",
            " [   1    0    2    0    2    0    0    0    0    0    0    4    3    2\n",
            "     1   14   37    3    1    2    0    0    1    0    3    4 2318    0\n",
            "     2]\n",
            " [   0    0    0    0    0    0    1    0    0    2    0    0    0    0\n",
            "     0    0    0    0    6    0    0    0    0    0    0    0    0 2391\n",
            "     0]\n",
            " [   0    3    8    3    0    0    2    0    0    1    6    9    0    0\n",
            "     0    4    2   28    4    9    2    1    2    3   11    0   19    0\n",
            "  2283]]\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.1\n",
            "   Trainable params: 14,877\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/272, Loss: 3.5544\n",
            "    Batch 100/272, Loss: 3.5501\n",
            "    Batch 200/272, Loss: 3.5737\n",
            "   Train: Loss=3.5579, Acc=0.038\n",
            "   Val:   Loss=3.5583, Acc=0.038  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/272, Loss: 3.5131\n",
            "    Batch 100/272, Loss: 3.5869\n",
            "    Batch 200/272, Loss: 3.5832\n",
            "   Train: Loss=3.5580, Acc=0.039\n",
            "   Val:   Loss=3.5582, Acc=0.038 \n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/272, Loss: 3.5713\n",
            "    Batch 100/272, Loss: 3.6264\n",
            "    Batch 200/272, Loss: 3.5166\n",
            "   Train: Loss=3.5580, Acc=0.039\n",
            "   Val:   Loss=3.5593, Acc=0.038 \n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.038\n",
            "Macro-F1 Score (Best Epoch): 0.0160\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[   0  828    0    0 1123   56    0    2    0    0   22   89    0    0\n",
            "     0    0    3   98    0    0    0    0    0  132    4    5   33    1\n",
            "     4]\n",
            " [   0  616    0    0 1131   19    0    0    0    0    5   16    0    0\n",
            "     0    0    3  385    0    0    3    0    0  177    1    5   30    0\n",
            "     9]\n",
            " [   0  360    0    0 1142  113    0   14    0    0   25  115    0    0\n",
            "     0    0    1  501    0    0    0    0    0   80    5   11   18    0\n",
            "    15]\n",
            " [   0  675    0    0  860  223    0    0    0    0   31   15    0    0\n",
            "     1    6    3  401    0    0   18    0    0   28    3   18    1    5\n",
            "   112]\n",
            " [   0  563    0    0 1129  132    0   11    0    0   62   89    0    0\n",
            "     0    0    4  180    0    0   24    0    4  122    5    7   36    1\n",
            "    31]\n",
            " [   0 1040    0    0  661  289    0    0    0    0   55   34    0    0\n",
            "     0    0    5  262    0    0    0    0    0   27    0    0    4    1\n",
            "    22]\n",
            " [   0  825    0    0  920   71    0   65    0    0   20  102    0    0\n",
            "     0    0    0  248    0    0    0    0    0   50   23    2   15    0\n",
            "    59]\n",
            " [   0  297    0    0 1689   36    0   69    0    0    5    2    0    0\n",
            "     0    0    0  174    0    0    1    0    0   98    1    0    3    0\n",
            "    25]\n",
            " [   0  711    0    0  725   82    0    5    0    0   36   11    0    0\n",
            "     0    0    8  656    0    0    0    0    0   64    8   30    3    0\n",
            "    61]\n",
            " [   0  735    0    0  918   99    0    6    0    0    7    8    0    0\n",
            "     0    0    0  373    0    0    0    0    0  127   41   67    0    0\n",
            "    19]\n",
            " [   0  522    0    0 1106  169    0   10    0    0   18    1    0    0\n",
            "     0    0    1  380    0    0    0    0    0   52   62   17    0    1\n",
            "    61]\n",
            " [   0  572    0    0  765  294    0   21    0    0   49  106    0    0\n",
            "     2    0    0  453    0    0    0    0    0   75   33   20    0    1\n",
            "     9]\n",
            " [   0 1218    0    0  361   22    0   38    0    0   19   21    0    0\n",
            "     0    0    1  390    0    0   21    0    4  186   46   43   10    1\n",
            "    19]\n",
            " [   0 1116    0    0  234   31    0   49    0    0   19   77    0    0\n",
            "     0    0    5  464    0    0   19    0    2  283   22   34   28    2\n",
            "    15]\n",
            " [   0  670    0    0  575  234    0   25    0    0   62   34    0    0\n",
            "     3    0   13  469    0    0   60    0    0   84   31   57    1   29\n",
            "    53]\n",
            " [   0  847    0    0  433  190    0   23    0    0   59  306    0    0\n",
            "     1    3    0  371    0    0   22    0    1   54    5   15    1    5\n",
            "    64]\n",
            " [   0  441    0    0  598  101    0   36    0    0   91  337    0    0\n",
            "     0    0    0  424    0    0    4    0    0  309    0   31    0   11\n",
            "    17]\n",
            " [   0  673    0    0 1053   28    2    1    0    0   97    7    0    0\n",
            "     4    0    0  285    0    0    3    0    0  130   23    9    6    1\n",
            "    78]\n",
            " [   0  479    0    0  682  197   13   25    0    0  243  231    0    0\n",
            "     1    0    1  256    0    0   58    0    0   82   17   39    0    3\n",
            "    73]\n",
            " [   0  875    0    0  461   80    0   31    0    0   75  279    0    0\n",
            "     0    0    0  327    0    0    5    0    2  134   43    7    0    3\n",
            "    78]\n",
            " [   0  791    0    0  906  135    1    8    0    0   69   57    0    0\n",
            "     0    0    0  290    0    0    2    0    1   81   12   16    1    9\n",
            "    21]\n",
            " [   0  949    0    0  830   81    1    3    0    0   74   59    0    0\n",
            "     0    0    7  357    0    0    0    0    0   14    2    4    0    0\n",
            "    19]\n",
            " [   0  774    0    0  772  101    0    7    0    0   51   58    0    0\n",
            "     0    0    5  581    0    0    0    0    0   21   11    8    6    0\n",
            "     5]\n",
            " [   0 1342    0    0  454   71    8   25    0    0   76  176    0    0\n",
            "     0    0    1  152    0    0    2    0    0   49    6   17    4    0\n",
            "    17]\n",
            " [   0  986    0    0  538   44    0   27    0    0    9  265    0    0\n",
            "     0    0    3  385    0    0    2    0    2   20   27    3    2   13\n",
            "    74]\n",
            " [   0  683    0    0  727   75    1    5    0    0   20  173    0    0\n",
            "     0    0    1  564    0    0    8    0    0   51   16    7    0    4\n",
            "    65]\n",
            " [   0  455    0    0  283  273    0   16    0    0   29  407    1    0\n",
            "     0    0    0  515    0    0   78    0    0  280    0   47    9    5\n",
            "     2]\n",
            " [   0  543    0    0 1404    4    0    7    0    0    0  195    0    0\n",
            "     0    0    0    2    0    0    0    0    0   60    0    3   89    2\n",
            "    91]\n",
            " [   0  692    0    0  439   57    0   38    0    0   53  825    0    0\n",
            "     0    0    0  197    0    0    0    0    0   33    5    5    0    0\n",
            "    56]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "head_loss_epoch_train = {}\n",
        "head_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_HEAD_ONLY = 3\n",
        "LR_HEAD = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = [64, 128, 256]\n",
        "\n",
        "for x in range(3):\n",
        "    #redefine model for each set of hyper paramaters\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    #Freeze Layers every Loop iteration\n",
        "    set_requires_grad(model, False)\n",
        "    set_requires_grad(model.fc, True)\n",
        "\n",
        "    #Create dictionary entry for losses by epoch across val and train\n",
        "    head_loss_epoch_train[x]=[]\n",
        "    head_loss_epoch_val[x]=[]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "    train_subset_ds,\n",
        "    batch_size=BATCH_SIZE[x],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if x == 0:\n",
        "        optimizer = optim.Adam(trainable_params, lr=LR_HEAD[x])\n",
        "    elif x==1:\n",
        "        optimizer = optim.SGD(trainable_params, lr=LR_HEAD[x])\n",
        "\n",
        "    elif x==2:\n",
        "        optim.Adagrad(trainable_params, lr=LR_HEAD[x])\n",
        "    print(f\"\\n Optimizer setup:\")\n",
        "    print(f\"   Learning rate: {LR_HEAD[x]}\")\n",
        "    print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Step 4: Training loop\n",
        "    print(\"\\n Training progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS_HEAD_ONLY + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS_HEAD_ONLY}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "        #Append losses for plotting purposes\n",
        "        head_loss_epoch_train[x].append(train_loss)\n",
        "        head_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            best_true_labels = true_labels_e\n",
        "            best_pred_labels = pred_labels_e\n",
        "            # Optional: Save best model\n",
        "            # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "        print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "        print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "    print(\"\\n Phase 1 Complete!\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "    #Best Epoch F1 Score\n",
        "    macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    #Confustion Matrix\n",
        "    conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "    print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "    print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(head_loss_epoch_train,head_loss_epoch_val )"
      ],
      "metadata": {
        "id": "VnoYeXlbo0NX",
        "outputId": "e3074bed-18b6-4913-b269-1eefbb906132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VnoYeXlbo0NX",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [0.8352512020078199, 0.2734742975783074, 0.18719716691422736], 1: [2.0909091646917934, 1.0848367992214774, 0.7846419388124313], 2: [3.5579178143643784, 3.5580359372325328, 3.5579533446520224]} {0: [0.3266822561724433, 0.18855492238340707, 0.13326176381659233], 1: [1.3304801023965593, 0.8731483811345594, 0.6749897766387326], 2: [3.558315995643879, 3.558240810701217, 3.5592981850415812]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5603562",
      "metadata": {
        "id": "f5603562"
      },
      "source": [
        "# TB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e4a92a97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4a92a97",
        "outputId": "647cd42e-596c-453c-f6d9-b7585a5f29bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 8,408,605\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/1088, Loss: 3.5769\n",
            "    Batch 100/1088, Loss: 0.0852\n",
            "    Batch 200/1088, Loss: 0.0288\n",
            "    Batch 300/1088, Loss: 0.0695\n",
            "    Batch 400/1088, Loss: 0.0043\n",
            "    Batch 500/1088, Loss: 0.0178\n",
            "    Batch 600/1088, Loss: 0.0835\n",
            "    Batch 700/1088, Loss: 0.0118\n",
            "    Batch 800/1088, Loss: 0.0041\n",
            "    Batch 900/1088, Loss: 0.0028\n",
            "    Batch 1000/1088, Loss: 0.0032\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0597, Acc=0.985\n",
            "   Val:   Loss=0.0012, Acc=1.000  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/1088, Loss: 0.0003\n",
            "    Batch 100/1088, Loss: 0.0006\n",
            "    Batch 200/1088, Loss: 0.0012\n",
            "    Batch 300/1088, Loss: 0.0003\n",
            "    Batch 400/1088, Loss: 0.0007\n",
            "    Batch 500/1088, Loss: 0.0167\n",
            "    Batch 600/1088, Loss: 0.0202\n",
            "    Batch 700/1088, Loss: 0.0226\n",
            "    Batch 800/1088, Loss: 0.0074\n",
            "    Batch 900/1088, Loss: 0.0012\n",
            "    Batch 1000/1088, Loss: 0.0027\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0123, Acc=0.997\n",
            "   Val:   Loss=0.0029, Acc=0.999 \n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/1088, Loss: 0.0006\n",
            "    Batch 100/1088, Loss: 0.0017\n",
            "    Batch 200/1088, Loss: 0.0001\n",
            "    Batch 300/1088, Loss: 0.0014\n",
            "    Batch 400/1088, Loss: 0.0000\n",
            "    Batch 500/1088, Loss: 0.0001\n",
            "    Batch 600/1088, Loss: 0.0022\n",
            "    Batch 700/1088, Loss: 0.0001\n",
            "    Batch 800/1088, Loss: 0.0001\n",
            "    Batch 900/1088, Loss: 0.0008\n",
            "    Batch 1000/1088, Loss: 0.0041\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0049, Acc=0.999\n",
            "   Val:   Loss=0.0234, Acc=0.996 \n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 1.000\n",
            "Macro-F1 Score (Best Epoch): 0.9997\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0 2400    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0 2399    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    1    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0 2400    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    7 2393    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0 2398    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0 2389    0    0   11    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0 2400    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0 2400    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0 2400    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400]]\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.01\n",
            "   Trainable params: 8,408,605\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/544, Loss: 3.4773\n",
            "    Batch 100/544, Loss: 1.3961\n",
            "    Batch 200/544, Loss: 0.6764\n",
            "    Batch 300/544, Loss: 0.3077\n",
            "    Batch 400/544, Loss: 0.2112\n",
            "    Batch 500/544, Loss: 0.1474\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.8282, Acc=0.873\n",
            "   Val:   Loss=0.1400, Acc=0.990  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/544, Loss: 0.1461\n",
            "    Batch 100/544, Loss: 0.1605\n",
            "    Batch 200/544, Loss: 0.1106\n",
            "    Batch 300/544, Loss: 0.0824\n",
            "    Batch 400/544, Loss: 0.0743\n",
            "    Batch 500/544, Loss: 0.0511\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0905, Acc=0.994\n",
            "   Val:   Loss=0.0483, Acc=0.998  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/544, Loss: 0.0489\n",
            "    Batch 100/544, Loss: 0.0514\n",
            "    Batch 200/544, Loss: 0.0375\n",
            "    Batch 300/544, Loss: 0.0556\n",
            "    Batch 400/544, Loss: 0.0294\n",
            "    Batch 500/544, Loss: 0.0275\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0413, Acc=0.998\n",
            "   Val:   Loss=0.0256, Acc=0.999  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.999\n",
            "Macro-F1 Score (Best Epoch): 0.9989\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0 2400    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0 2399    1    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    1 2399    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    4    0    0    0 2395    1    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    1 2399    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0 2399    0    0    0\n",
            "     0    0    0    0    0    0    0    1    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 2398    0    0\n",
            "     0    0    0    0    0    2    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0 2393    7\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    7 2393\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    1    0    0    0    0    0    0\n",
            "     0 2398    0    0    0    0    0    0    0    0    0    0    1    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    1 2399    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0 2397    0    0    2    0    0    1    0    0    0    0\n",
            "     0]\n",
            " [   2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2398    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    1    0    0    0\n",
            "     0    0    0    5    0    0 2394    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    2    0    0    0\n",
            "     0    0    0    0    0    0    4 2387    5    2    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0]\n",
            " [   1    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    3    0    5    1    0 2390    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    1    0    0    0    0 2399    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    2    0    0    0 2398    0    0\n",
            "     0]\n",
            " [   0    0    2    0    0    0    0    0    0    0    0    0    0    0\n",
            "     1    0    0    0    0    1    0    0    0    0    0    4 2392    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    1    0    0    0    0    0    0    0    0    0    0\n",
            "  2399]]\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.1\n",
            "   Trainable params: 8,408,605\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/272, Loss: 3.5119\n",
            "    Batch 100/272, Loss: 3.4789\n",
            "    Batch 200/272, Loss: 3.4768\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5178, Acc=0.029\n",
            "   Val:   Loss=3.5167, Acc=0.029  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/272, Loss: 3.5486\n",
            "    Batch 100/272, Loss: 3.5670\n",
            "    Batch 200/272, Loss: 3.5408\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5175, Acc=0.028\n",
            "   Val:   Loss=3.5173, Acc=0.028 \n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/272, Loss: 3.5275\n",
            "    Batch 100/272, Loss: 3.5639\n",
            "    Batch 200/272, Loss: 3.5677\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5177, Acc=0.029\n",
            "   Val:   Loss=3.5187, Acc=0.029 \n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.029\n",
            "Macro-F1 Score (Best Epoch): 0.0070\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[   0   16    2    0    0    0    0    0    0    0    0   37    0   95\n",
            "     0    0    0   46 1774    0    0    0    0    0   24  126    0  279\n",
            "     1]\n",
            " [   0    6    6    0    0    0    0    0    0    0    0    0    0  230\n",
            "     0    0    0   42 1717    0    0    0    0    0   51  304    6   35\n",
            "     3]\n",
            " [   0    2   14    0    0    0    0    0    0    0    0    6    0   18\n",
            "     0    0    0    0 1597    5    0    0    2    0   13  722    3   13\n",
            "     5]\n",
            " [   0   10   31    0    0    0    0    0    0    0    0   32    0   25\n",
            "     0    0    0    6 1195    0    1    0    0    0   19 1068    0   11\n",
            "     2]\n",
            " [   0  117    5    0    0    0    0    0    0    0    0    9    0  244\n",
            "     0    0    0   18 1327    0    0    0    0    0  132  470    0   78\n",
            "     0]\n",
            " [   0    1   19    0    0    0    0    0    0    3    0    4    0   81\n",
            "     0    0    0   26 1573   26    1    0    0    0   89  493    0   83\n",
            "     1]\n",
            " [   0    6   59    0    0    0    4    0    0    0    0    0    0   31\n",
            "     0    0    0   28 1429    2    0    0    1    0  175  642    0   21\n",
            "     2]\n",
            " [   0    0   12    0    0    0    0    0    0    1    0    0    0   21\n",
            "     0    0    0   18 1661    0    1    0    2    0  218  451    0    0\n",
            "    15]\n",
            " [   0   15    0    0    0    0    3    0    0    0    0    0    0   33\n",
            "     0    0    0  158 1567    1    0    0    0    0   25  556    3   39\n",
            "     0]\n",
            " [   0    3    0    0    0    0    5    0    0    0    0    0    0    9\n",
            "     0    0    0   40 1173    0    0    0    0    0   49 1107    0   12\n",
            "     2]\n",
            " [   0    7   47    0    0    0    1    0    0    0    0    0    0   37\n",
            "     0    0    0   99 1326    0    0    0    0    0  294  520    0   61\n",
            "     8]\n",
            " [   0    2   10    0    0    1    0    0    0    0    0    1    0    8\n",
            "     0    0    0   71 1190    0    0    0    0    0  270  809    0   30\n",
            "     8]\n",
            " [   0    7    2    0    0    0    0    0    0    0    0    0    0   81\n",
            "     0    0    0   16 1191    0    0    0    0    0   33 1014    0   43\n",
            "    13]\n",
            " [   0   16    3    0    0    0    0    0    0    0    0   11    2   64\n",
            "     0    0    0   74 1251    0    0    0    0    0   65  843    0   69\n",
            "     2]\n",
            " [   0   13    9    0    0    0    0    0    0    0    0    5    0   35\n",
            "     0    0    0    8  833    0    0    0    0    0   17 1457    0   18\n",
            "     5]\n",
            " [   0    1    1    0    0    0    0    0    0    0    0   18    0   35\n",
            "     0    0    0   11  648    5    0    0    1    0    4 1623    3   47\n",
            "     3]\n",
            " [   0    2    0    0    0    0    0    0    0    0    0   31    0    1\n",
            "     0    0    0   15 1481    0    0    0    0    0   29  809    0   30\n",
            "     2]\n",
            " [   1    4    1    0    0    0    0    0    0    1    0    0    0   70\n",
            "     0    0    0    7 1606    0    0    0    0    0   15  675    1   17\n",
            "     2]\n",
            " [   0   13    1    0    0    0    3    0    0    0    0    0    0   55\n",
            "     0    0    0   38  880    0    0    0    0    0    2 1346    0   62\n",
            "     0]\n",
            " [   0   10    0    0    0    0    0    0    0    0    0    7    0   61\n",
            "     0    0    0   17 1281    0    0    0    5    0  111  762    0  142\n",
            "     4]\n",
            " [   0    1    1    0    0    0    0    0    0    0    0    0    0   31\n",
            "     0    0    0   10 1532    0    0    0    0    0    5  802    0   18\n",
            "     0]\n",
            " [   0   17   26    0    0    0    0    0    0    0    0    0    1   44\n",
            "     0    0    0   36 1213    0    0    0    6    0   64  921    0   43\n",
            "    29]\n",
            " [   0   15    2    0    0    0    0    0    0    3    0    0    0   75\n",
            "     0   11    0   91 1009    3    0    0    0    0  109  921    0   56\n",
            "   105]\n",
            " [   0   20    0    0    0    0    1    0    0    1    0    0    1   90\n",
            "     0    0    0   15 1462    0    0    0    0    0    3  745    4   57\n",
            "     1]\n",
            " [   0    1    0    0    0    0    0    0    0    3    0    0    0   78\n",
            "     0    0    0    9 1197    0    0    0    0    0   76 1022    0   10\n",
            "     4]\n",
            " [   0   68    1    0    0    0    0    0    0    0    0    0    0    9\n",
            "     0    0    0    0 1295    0    0    0    1    0   37  931    0   58\n",
            "     0]\n",
            " [   0   13    0    0    0    0    0    0    0    1    0    0    0    0\n",
            "     0    0    0   14  611    0    0    0   13    0   20 1701    2   18\n",
            "     7]\n",
            " [   0    0   40    0    0    0    0    0    0    0    0    0    0    4\n",
            "     0    0    0    0  605    0    0    0    0    0   25 1725    1    0\n",
            "     0]\n",
            " [   0    0    2    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0   16 1351    0    0    0    0    0   17  993    0   21\n",
            "     0]]\n"
          ]
        }
      ],
      "source": [
        "#Redefine model for phase 2\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TB_loss_epoch_train = {}\n",
        "TB_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TB = 3\n",
        "LR_TB = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [64, 128, 256]\n",
        "\n",
        "for x in range(3):\n",
        "  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "  model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "  model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "  # Step 1: Freeze entire model\n",
        "  print(\"\\n Freezing all layers...\")\n",
        "  set_requires_grad(model, False)\n",
        "\n",
        "  # Step 2: Unfreeze the classifier head and last layer\n",
        "  print(\"\\n Unfreezing classifier head...\")\n",
        "  set_requires_grad(model.fc, True)\n",
        "  set_requires_grad(model.layer4, True)\n",
        "\n",
        "  #Dictionary entry\n",
        "  TB_loss_epoch_train[x]=[]\n",
        "  TB_loss_epoch_val[x]=[]\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  val_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  if x == 0:\n",
        "      optimizer = optim.Adam(trainable_params, lr=LR_TB[x])\n",
        "  elif x==1:\n",
        "      optimizer = optim.SGD(trainable_params, lr=LR_TB[x])\n",
        "\n",
        "  elif x==2:\n",
        "      optim.Adagrad(trainable_params, lr=LR_TB[x])\n",
        "  print(f\"\\n Optimizer setup:\")\n",
        "  print(f\"   Learning rate: {LR_TB[x]}\")\n",
        "  print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "  # Step 4: Training loop\n",
        "  print(\"\\n Training progress:\")\n",
        "  print(\"-\" * 60)\n",
        "\n",
        "  best_val_acc = 0.0\n",
        "  for epoch in range(1, EPOCHS_TB + 1):\n",
        "      print(f\"\\nEpoch {epoch}/{EPOCHS_TB}\")\n",
        "\n",
        "      # Train\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "      # Validate\n",
        "      val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "      #Append losses for plotting purposes\n",
        "      TB_loss_epoch_train[x].append(train_loss)\n",
        "      TB_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "      # Track best model\n",
        "      if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "\n",
        "          best_true_labels = true_labels_e\n",
        "          best_pred_labels = pred_labels_e\n",
        "          # Optional: Save best model\n",
        "          # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "      print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "      print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "        f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "  print(\"\\n Phase 1 Complete!\")\n",
        "  print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "  #Best Epoch F1 Score\n",
        "  macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                      average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "  #Confustion Matrix\n",
        "  conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                  labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "  print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "  print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "  print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TB_loss_epoch_train, TB_loss_epoch_val)"
      ],
      "metadata": {
        "id": "n85TVG8mx96A",
        "outputId": "c7985cb0-1c11-4e60-c194-67247e59d781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n85TVG8mx96A",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [0.059708090944432426, 0.012279382735676796, 0.0048836680524463995], 1: [0.8281598437243495, 0.09050373091094795, 0.04134318123004217], 2: [3.5177802151647106, 3.5175116854700548, 3.517668616525058]} {0: [0.0012200415094989223, 0.002874514754669575, 0.023411180782091708], 1: [0.1399805306018084, 0.04832170251799726, 0.025599398795386842], 2: [3.5167479360514675, 3.517269561921043, 3.5186618635024147]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12982a6b",
      "metadata": {
        "id": "12982a6b"
      },
      "source": [
        "# Model TC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c7132526",
      "metadata": {
        "id": "c7132526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb544bde-0723-4b89-fe4d-e1d60a53f355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "  Sequential: 2,099,712 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 10,508,317\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/1088, Loss: 3.6309\n",
            "    Batch 100/1088, Loss: 0.1135\n",
            "    Batch 200/1088, Loss: 0.0029\n",
            "    Batch 300/1088, Loss: 0.0321\n",
            "    Batch 400/1088, Loss: 0.2207\n",
            "    Batch 500/1088, Loss: 0.0082\n",
            "    Batch 600/1088, Loss: 0.0027\n",
            "    Batch 700/1088, Loss: 0.0018\n",
            "    Batch 800/1088, Loss: 0.0467\n",
            "    Batch 900/1088, Loss: 0.0090\n",
            "    Batch 1000/1088, Loss: 0.0013\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0507, Acc=0.987\n",
            "   Val:   Loss=0.0015, Acc=1.000  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/1088, Loss: 0.0234\n",
            "    Batch 100/1088, Loss: 0.0005\n",
            "    Batch 200/1088, Loss: 0.0002\n",
            "    Batch 300/1088, Loss: 0.0003\n",
            "    Batch 400/1088, Loss: 0.0001\n",
            "    Batch 500/1088, Loss: 0.0004\n",
            "    Batch 600/1088, Loss: 0.0004\n",
            "    Batch 700/1088, Loss: 0.1856\n",
            "    Batch 800/1088, Loss: 0.0020\n",
            "    Batch 900/1088, Loss: 0.0163\n",
            "    Batch 1000/1088, Loss: 0.0003\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0145, Acc=0.996\n",
            "   Val:   Loss=0.0005, Acc=1.000  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/1088, Loss: 0.0001\n",
            "    Batch 100/1088, Loss: 0.0002\n",
            "    Batch 200/1088, Loss: 0.0020\n",
            "    Batch 300/1088, Loss: 0.0004\n",
            "    Batch 400/1088, Loss: 0.0198\n",
            "    Batch 500/1088, Loss: 0.0726\n",
            "    Batch 600/1088, Loss: 0.0019\n",
            "    Batch 700/1088, Loss: 0.0233\n",
            "    Batch 800/1088, Loss: 0.0004\n",
            "    Batch 900/1088, Loss: 0.0003\n",
            "    Batch 1000/1088, Loss: 0.0001\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0093, Acc=0.998\n",
            "   Val:   Loss=0.0001, Acc=1.000  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 1.000\n",
            "Macro-F1 Score (Best Epoch): 1.0000\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0 2400    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0 2400    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0 2400    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0 2400    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0 2400    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0 2400    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0 2400    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0 2400    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400]]\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "  Sequential: 2,099,712 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.01\n",
            "   Trainable params: 10,508,317\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/544, Loss: 3.5698\n",
            "    Batch 100/544, Loss: 1.0107\n",
            "    Batch 200/544, Loss: 0.3275\n",
            "    Batch 300/544, Loss: 0.1715\n",
            "    Batch 400/544, Loss: 0.1240\n",
            "    Batch 500/544, Loss: 0.0680\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.6076, Acc=0.912\n",
            "   Val:   Loss=0.0594, Acc=0.997  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/544, Loss: 0.0589\n",
            "    Batch 100/544, Loss: 0.0450\n",
            "    Batch 200/544, Loss: 0.0662\n",
            "    Batch 300/544, Loss: 0.0344\n",
            "    Batch 400/544, Loss: 0.0253\n",
            "    Batch 500/544, Loss: 0.0185\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0405, Acc=0.998\n",
            "   Val:   Loss=0.0200, Acc=1.000  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/544, Loss: 0.0228\n",
            "    Batch 100/544, Loss: 0.0310\n",
            "    Batch 200/544, Loss: 0.0373\n",
            "    Batch 300/544, Loss: 0.0153\n",
            "    Batch 400/544, Loss: 0.0154\n",
            "    Batch 500/544, Loss: 0.0175\n",
            "Reached Evaluate\n",
            "   Train: Loss=0.0184, Acc=1.000\n",
            "   Val:   Loss=0.0110, Acc=1.000  New best!\n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 1.000\n",
            "Macro-F1 Score (Best Epoch): 0.9999\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0 2400    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0 2400    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0 2400    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0 2398    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    1 2399\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2400    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 2400    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0 2399    0    0    1    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2400    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2400    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0 2400    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0 2399    1    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 2400    0    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    1    0    0    0    0 2399    0    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0 2400    0    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0 2400    0    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
            "     0    0    0    0    1    0    0    0    0    0    0    1 2396    0\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0 2400\n",
            "     0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  2400]]\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters FROZEN\n",
            "\n",
            " Unfreezing classifier head...\n",
            "  Linear: 14,877 parameters UNFROZEN (trainable)\n",
            "  Sequential: 8,393,728 parameters UNFROZEN (trainable)\n",
            "  Sequential: 2,099,712 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.1\n",
            "   Trainable params: 10,508,317\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/272, Loss: 3.5807\n",
            "    Batch 100/272, Loss: 3.5550\n",
            "    Batch 200/272, Loss: 3.4670\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5360, Acc=0.033\n",
            "   Val:   Loss=3.5358, Acc=0.033  New best!\n",
            "\n",
            "Epoch 2/3\n",
            "    Batch 0/272, Loss: 3.5592\n",
            "    Batch 100/272, Loss: 3.4727\n",
            "    Batch 200/272, Loss: 3.5236\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5360, Acc=0.032\n",
            "   Val:   Loss=3.5368, Acc=0.033  New best!\n",
            "\n",
            "Epoch 3/3\n",
            "    Batch 0/272, Loss: 3.5255\n",
            "    Batch 100/272, Loss: 3.5686\n",
            "    Batch 200/272, Loss: 3.4782\n",
            "Reached Evaluate\n",
            "   Train: Loss=3.5357, Acc=0.032\n",
            "   Val:   Loss=3.5355, Acc=0.032 \n",
            "\n",
            " Phase 1 Complete!\n",
            "   Best validation accuracy: 0.033\n",
            "Macro-F1 Score (Best Epoch): 0.0099\n",
            "\n",
            "Confusion Matrix (True vs. Predicted):\n",
            "\n",
            "[[   1    0    0   68  817    0    3   84    2   89    0    0    3 1177\n",
            "     0    0  140    2    0    5    2    0    0    0    0    0    6    1\n",
            "     0]\n",
            " [   1    0    0   83  459    0    5   76    7  107   16    3   24 1603\n",
            "     0    0    2    0    0    1    0    2    0    0    0    0    2    9\n",
            "     0]\n",
            " [   0    5    0   17  298    0    1   21   22  501    4    0    0 1417\n",
            "     0    0   21    0    0    4    0   15    0    0    0    0   73    1\n",
            "     0]\n",
            " [   0   22    0   24  335    0    2   18   18  123    1    0    0 1820\n",
            "     0    0   19    2    0    0    0    9    0    0    0    0    7    0\n",
            "     0]\n",
            " [   3    0    0   73  937    0    0   60    5    6    1    0    0 1299\n",
            "     0    0    2    1    0    0    0    9    0    0    0    0    4    0\n",
            "     0]\n",
            " [   0    0    0    3  631    0    1   67    1   56    0    2    0 1507\n",
            "     0    0    1    2    0    1    0  128    0    0    0    0    0    0\n",
            "     0]\n",
            " [   1   10    1   32  921    0    0    7   17   67    1    2    0 1160\n",
            "     0    0   18    1    0    1    0    6    0    0    3    0  139   13\n",
            "     0]\n",
            " [   1   12    0    3  728    0    0  125    3  224    5    0    0 1172\n",
            "     0    0    5    1    0    3    0   12    0    0    5    0   63   38\n",
            "     0]\n",
            " [   0    3    2   14  968    0    2    9    6   32    2    0    0 1353\n",
            "     0    0    1    3    0    0    0    0    0    0    0    0    4    1\n",
            "     0]\n",
            " [   0    0    4   37 1101    0    2   21   17   75    1    0    0 1110\n",
            "     0    0   19    1    0    0    0    5    1    0    1    0    4    1\n",
            "     0]\n",
            " [   0    0    1    6  346    0    2    5   55   49    1    0    0 1840\n",
            "     0    0   33    4    0    0    0   52    0    0    2    0    4    0\n",
            "     0]\n",
            " [   0    2    0    3  588    0    0    1   23  113    0    0    0 1641\n",
            "     1    0   11    1    0    0    0   11    0    0    0    0    5    0\n",
            "     0]\n",
            " [   0    2    0   54  811    0    0    0   13   76    1    0    0 1390\n",
            "     0    0   20    0    0    0    1    3    3    0    1    0   11   14\n",
            "     0]\n",
            " [   0    6    3   58 1003    0    1    5   56  202    0    3    0 1039\n",
            "     0    0    4    2    0    0    0    0   10    0    0    0    8    0\n",
            "     0]\n",
            " [   1    0    0   57  714    0    2    2   68  105    1    0    0 1343\n",
            "     0    0   94    3    0    0    2    4    0    0    0    1    3    0\n",
            "     0]\n",
            " [   0    5    2   61  337    0    1   10    2  924    0    0    0  877\n",
            "     0    0   63    0    0    1    0   55    0    0    0    0   46   15\n",
            "     1]\n",
            " [   0    0    1   34  462    0    0   36   35  533    1    0    0 1127\n",
            "     0    0    6    0    0    3    0   16    0    0    0    0  138    6\n",
            "     2]\n",
            " [   0    0    2   31  880    0    1    9   38   15    0    0    0 1370\n",
            "     0    0   10    0    0    0    0   28    0    0    0    0   14    2\n",
            "     0]\n",
            " [   0    3    0   64 1417    0    0   30   12   28    0    0    0  816\n",
            "     0    0    3    1    0    0    0    1    0    0    0    0   20    5\n",
            "     0]\n",
            " [   0    0    0   17 1373    0    4    0   11  102    1    0    0  852\n",
            "     0    0    9    3    0    0    1    4    0    0    0    0   21    2\n",
            "     0]\n",
            " [   1    1   10   22  721    0    2   36   26   52    1    0    0 1496\n",
            "     0    0    7    1    0    0    1   15    0    0    0    1    5    2\n",
            "     0]\n",
            " [   0    0    1    7  584    0    0   13   10   31    0    0    0 1700\n",
            "     0    0    2    1    0    1    0   41    1    0    0    0    7    1\n",
            "     0]\n",
            " [   1    0    2   22  314    0    3   13   15   21    0    0    0 1973\n",
            "     0    0    3    3    0    0    0    9    0    0    0    0   12    9\n",
            "     0]\n",
            " [   0    0    3   36 1030    0    1    2   10   55    0    1    0 1239\n",
            "     0    0    1    4    0    1    0    5    0    0    0    0    9    3\n",
            "     0]\n",
            " [   0    8    2    9 1120    0    0    2    7   80    0    0    0 1069\n",
            "     0    0   13    3    0    7    3   62    0    0    0    1   10    4\n",
            "     0]\n",
            " [   0    7    2   26 1113    0    1   13   13   55    0    0    0 1074\n",
            "     0    0   37    1    0    0    0   28    0    0    0    0   13   17\n",
            "     0]\n",
            " [   0    1   13   41  503    0   17   65   15  731    1    1    0  917\n",
            "     0    0   32    0    0    5    0   44    1    0    0    0    7    6\n",
            "     0]\n",
            " [  77    0    0  441  131    0    1  800   38  223    3    7    0  403\n",
            "     0    5  106    0    0    3    8  106    0    0    0    0   22   26\n",
            "     0]\n",
            " [   0    3    4   52 1406    0    9  141    6   75    0    4    0  569\n",
            "     0    0    2    0    0    5    1   49    0    0    0    1   62   11\n",
            "     0]]\n"
          ]
        }
      ],
      "source": [
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "TC_loss_epoch_train = {}\n",
        "TC_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_TC = 3\n",
        "LR_TC = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "BATCH_SIZE = [64, 128, 256]\n",
        "\n",
        "for x in range(3):\n",
        "  #Redefine model for phase 3\n",
        "  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "  model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "  model = model.to(DEVICE)\n",
        "\n",
        "  # Step 1: Freeze entire model\n",
        "  print(\"\\n Freezing all layers...\")\n",
        "  set_requires_grad(model, False)\n",
        "\n",
        "  # Step 2: Unfreeze the classifier head and last 2 layers\n",
        "  print(\"\\n Unfreezing classifier head...\")\n",
        "  set_requires_grad(model.fc, True)\n",
        "  set_requires_grad(model.layer4, True)\n",
        "  set_requires_grad(model.layer3, True)\n",
        "\n",
        "  TC_loss_epoch_train[x]=[]\n",
        "  TC_loss_epoch_val[x]=[]\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  val_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  if x == 0:\n",
        "      optimizer = optim.Adam(trainable_params, lr=LR_TC[x])\n",
        "  elif x==1:\n",
        "      optimizer = optim.SGD(trainable_params, lr=LR_TC[x])\n",
        "\n",
        "  elif x==2:\n",
        "      optim.Adagrad(trainable_params, lr=LR_TC[x])\n",
        "  print(f\"\\n Optimizer setup:\")\n",
        "  print(f\"   Learning rate: {LR_TC[x]}\")\n",
        "  print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "  # Step 4: Training loop\n",
        "  print(\"\\n Training progress:\")\n",
        "  print(\"-\" * 60)\n",
        "\n",
        "  best_val_acc = 0.0\n",
        "  for epoch in range(1, EPOCHS_TC + 1):\n",
        "      print(f\"\\nEpoch {epoch}/{EPOCHS_TC}\")\n",
        "\n",
        "      # Train\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "      # Validate\n",
        "      val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "      #Append losses for plotting purposes\n",
        "      TC_loss_epoch_train[x].append(train_loss)\n",
        "      TC_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "      # Track best model\n",
        "      if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "\n",
        "          best_true_labels = true_labels_e\n",
        "          best_pred_labels = pred_labels_e\n",
        "          # Optional: Save best model\n",
        "          # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "      print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "      print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "        f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "  print(\"\\n Phase 1 Complete!\")\n",
        "  print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "\n",
        "  #Best Epoch F1 Score\n",
        "  macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                      average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "  #Confustion Matrix\n",
        "  conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                  labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "  print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "  print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "  print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aea79058",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aea79058",
        "outputId": "c5feb0d6-406b-4966-f5ba-35d7828a4181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [0.05071969673313179, 0.014459311586618022, 0.009326013957040555], 1: [0.6076134274711554, 0.04045207892683731, 0.018384167248832767], 2: [3.536001625280271, 3.536001447589918, 3.5357357815490373]} {0: [0.0015304048959635488, 0.0004787045624868171, 0.0001014566563958658], 1: [0.059416782148610583, 0.019992381551827507, 0.010959330131353319], 2: [3.535817420126378, 3.5368056740157905, 3.535481838905948]}\n"
          ]
        }
      ],
      "source": [
        "print(TC_loss_epoch_train, TC_loss_epoch_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ac2759",
      "metadata": {
        "id": "64ac2759"
      },
      "source": [
        "# Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "13024128",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "13024128",
        "outputId": "d20fd693-ff79-4387-e04e-8f82ca7abf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PHASE 1: HEAD-ONLY FINE-TUNING\n",
            "============================================================\n",
            "\n",
            " Freezing all layers...\n",
            "  ResNet: 11,191,389 parameters UNFROZEN (trainable)\n",
            "  ResNet: 11,191,389 parameters UNFROZEN (trainable)\n",
            "\n",
            " Optimizer setup:\n",
            "   Learning rate: 0.001\n",
            "   Trainable params: 11,191,389\n",
            "\n",
            " Training progress:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/3\n",
            "    Batch 0/2175, Loss: 3.4135\n",
            "    Batch 100/2175, Loss: 0.2010\n",
            "    Batch 200/2175, Loss: 0.0910\n",
            "    Batch 300/2175, Loss: 0.0178\n",
            "    Batch 400/2175, Loss: 0.1703\n",
            "    Batch 500/2175, Loss: 0.0149\n",
            "    Batch 600/2175, Loss: 0.0350\n",
            "    Batch 700/2175, Loss: 0.0139\n",
            "    Batch 800/2175, Loss: 0.0051\n",
            "    Batch 900/2175, Loss: 0.1146\n",
            "    Batch 1000/2175, Loss: 0.0016\n",
            "    Batch 1100/2175, Loss: 0.0376\n",
            "    Batch 1200/2175, Loss: 0.1760\n",
            "    Batch 1300/2175, Loss: 0.0054\n",
            "    Batch 1400/2175, Loss: 0.1164\n",
            "    Batch 1500/2175, Loss: 0.0171\n",
            "    Batch 1600/2175, Loss: 0.0012\n",
            "    Batch 1700/2175, Loss: 0.0044\n",
            "    Batch 1800/2175, Loss: 0.0550\n",
            "    Batch 1900/2175, Loss: 0.0095\n",
            "    Batch 2000/2175, Loss: 0.0102\n",
            "    Batch 2100/2175, Loss: 0.0101\n",
            "Reached Evaluate\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4220735068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;31m#Append losses for plotting purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0mfull_loss_epoch_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0mfull_loss_epoch_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ],
      "source": [
        "#Redefine model for phase 4\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "#This is going to hold the losses for the different hyper paramate combinations\n",
        "full_loss_epoch_train = {}\n",
        "full_loss_epoch_val ={}\n",
        "# Hyperparameters for Phase 1\n",
        "\n",
        "EPOCHS_full = 3\n",
        "LR_full = [1e-3, 1e-2, .1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Freeze entire model\n",
        "print(\"\\n Freezing all layers...\")\n",
        "set_requires_grad(model, True)\n",
        "\n",
        "# Step 2: None Frozen\n",
        "\n",
        "\n",
        "# Step 3: Create optimizer for ONLY trainable parameters\n",
        "# filter() ensures we only optimize parameters with requires_grad=True\n",
        "\n",
        "BATCH_SIZE = [32, 64, 128]\n",
        "\n",
        "for x in range(3):\n",
        "\n",
        "  #Redefine model for phase 4\n",
        "  model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "  model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "  model = model.to(DEVICE)\n",
        "  TC_loss_epoch_train[x]=[]\n",
        "  TC_loss_epoch_val[x]=[]\n",
        "\n",
        "  #no layers frozen\n",
        "  set_requires_grad(model, True)\n",
        "  train_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  val_loader = DataLoader(\n",
        "  train_subset_ds,\n",
        "  batch_size=BATCH_SIZE[x],\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  pin_memory=True\n",
        "  )\n",
        "\n",
        "  trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  if x == 0:\n",
        "      optimizer = optim.Adam(trainable_params, lr=LR_full[x])\n",
        "  elif x==1:\n",
        "      optimizer = optim.SGD(trainable_params, lr=LR_full[x])\n",
        "\n",
        "  elif x==2:\n",
        "      optim.Adagrad(trainable_params, lr=LR_full[x])\n",
        "  print(f\"\\n Optimizer setup:\")\n",
        "  print(f\"   Learning rate: {LR_full[x]}\")\n",
        "  print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "  # Step 4: Training loop\n",
        "  print(\"\\n Training progress:\")\n",
        "  print(\"-\" * 60)\n",
        "\n",
        "  best_val_acc = 0.0\n",
        "  for epoch in range(1, EPOCHS_full + 1):\n",
        "      print(f\"\\nEpoch {epoch}/{EPOCHS_full}\")\n",
        "\n",
        "      # Train\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "      # Validate\n",
        "      val_loss, val_acc, true_labels_e, pred_labels_e = evaluate(model, val_loader)\n",
        "\n",
        "      #Append losses for plotting purposes\n",
        "      full_loss_epoch_train[x].append(train_loss)\n",
        "      full_loss_epoch_val[x].append(val_loss)\n",
        "\n",
        "      # Track best model\n",
        "      if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "\n",
        "          best_true_labels = true_labels_e\n",
        "          best_pred_labels = pred_labels_e\n",
        "          # Optional: Save best model\n",
        "          # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
        "\n",
        "      print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
        "      print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
        "        f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
        "\n",
        "  print(\"\\n Phase 1 Complete!\")\n",
        "  print(f\"   Best validation accuracy: {best_val_acc:.3f}\")\n",
        "#Best Epoch F1 Score\n",
        "  macro_f1 = f1_score(best_true_labels, best_pred_labels,\n",
        "                        average='macro', labels=list(range(len(full_train_ds.classes))))\n",
        "\n",
        "#Confustion Matrix\n",
        "  conf_matrix = confusion_matrix(best_true_labels, best_pred_labels,\n",
        "                                   labels=list(range(len(full_train_ds.classes))))\n",
        "  print(f\"Macro-F1 Score (Best Epoch): {macro_f1:.4f}\")\n",
        "  print(\"\\nConfusion Matrix (True vs. Predicted):\\n\")\n",
        "  print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_loss_epoch_train, full_loss_epoch_val)"
      ],
      "metadata": {
        "id": "BQoeGulDyMsU"
      },
      "id": "BQoeGulDyMsU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}