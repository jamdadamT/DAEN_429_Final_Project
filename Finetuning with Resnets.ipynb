{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9438beb8",
   "metadata": {},
   "source": [
    "# Demo toy dataset: CIFAR-10 (32×32) — upscaled to 224×224 for pretrained ResNet-18.\n",
    "### Student project: Replace CIFAR-10 with ASL Alphabet (Kaggle) (200×200 RGB, 29 classes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd338df",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "Understand transfer learning — why use pretrained models and when to freeze layers\n",
    "\n",
    "Master the ResNet-18 architecture — layers, blocks, skip connections, and their purpose\n",
    "\n",
    "Control fine-tuning granularity — freeze/unfreeze specific layers strategically\n",
    "\n",
    "Apply best practices — proper data preprocessing, normalization, and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358f772",
   "metadata": {},
   "source": [
    "## 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b6277ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard PyTorch + Torchvision stack\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Reproducibility (essential for research and debugging)\n",
    "import random\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Note: For complete reproducibility, you may also need:\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device (GPU if available)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e8afc",
   "metadata": {},
   "source": [
    "## 2) Understanding ResNet-18 Architecture\n",
    "### What Makes ResNets Special?\n",
    "The Problem: Deep networks suffer from vanishing gradients — as networks get deeper, gradients become exponentially small, making training nearly impossible.\n",
    "\n",
    "The Solution: Residual connections (skip connections) allow gradients to flow directly through shortcuts, enabling training of very deep networks.\n",
    "\n",
    "ResNet-18 Architecture Breakdown\n",
    "ResNet-18 has 18 layers with learnable weights. Here's the complete structure:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaff2e6b",
   "metadata": {},
   "source": [
    "Input (3×224×224)\n",
    "    ↓\n",
    "[Initial Processing Block]\n",
    "├── conv1: 7×7 conv, 64 filters, stride=2  → (64×112×112)\n",
    "├── bn1: Batch Normalization\n",
    "├── relu: ReLU activation\n",
    "├── maxpool: 3×3 max pool, stride=2        → (64×56×56)\n",
    "    ↓\n",
    "[Residual Stages]\n",
    "├── layer1: 2 BasicBlocks, 64 filters      → (64×56×56)\n",
    "├── layer2: 2 BasicBlocks, 128 filters     → (128×28×28) \n",
    "├── layer3: 2 BasicBlocks, 256 filters     → (256×14×14)\n",
    "├── layer4: 2 BasicBlocks, 512 filters     → (512×7×7)\n",
    "    ↓\n",
    "[Classification Head]\n",
    "├── avgpool: Adaptive Average Pool         → (512×1×1)\n",
    "├── fc: Fully Connected (512 → 1000)       → (1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee6f9d",
   "metadata": {},
   "source": [
    "## What's a BasicBlock?\n",
    "Each BasicBlock learns a residual function F(x) that's added to the input:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bad88575",
   "metadata": {},
   "source": [
    " Input x ──────────────────────────────────┐\n",
    "    ↓                                      │\n",
    "conv 3×3 → BN → ReLU → conv 3×3 → BN       │\n",
    "    ↓                                      │\n",
    "    └──────────────── Addition ←───────────┘\n",
    "                         ↓\n",
    "                      ReLU → Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6a42a",
   "metadata": {},
   "source": [
    "The block learns: Output = F(x) + x\n",
    "Where F(x) is the residual (the \"difference\" to learn), making it easier to learn identity mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c593ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "\n",
      " Parameters per module:\n",
      "  conv1     :        9,408 params\n",
      "  bn1       :          128 params\n",
      "  relu      :            0 params\n",
      "  maxpool   :            0 params\n",
      "  layer1    :      147,968 params\n",
      "  layer2    :      525,568 params\n",
      "  layer3    :    2,099,712 params\n",
      "  layer4    :    8,393,728 params\n",
      "  avgpool   :            0 params\n",
      "  fc        :      513,000 params\n",
      "****************************************************************************************************\n",
      "\n",
      "  TOTAL     : 11,689,512 params\n"
     ]
    }
   ],
   "source": [
    "# Let's examine a fresh ResNet-18 pretrained on ImageNet\n",
    "res18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Print the full architecture\n",
    "print(res18)\n",
    "print(\"*\"*100)\n",
    "\n",
    "# Count parameters in each stage\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters by module\"\"\"\n",
    "    param_counts = {}\n",
    "    for name, module in model.named_children():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        param_counts[name] = f\"{params:,}\"\n",
    "    return param_counts\n",
    "\n",
    "print(\"\\n Parameters per module:\")\n",
    "for module, count in count_parameters(res18).items():\n",
    "    print(f\"  {module:10s}: {count:>12s} params\")\n",
    "print(\"*\"*100)\n",
    "total_params = sum(p.numel() for p in res18.parameters())\n",
    "print(f\"\\n  {'TOTAL':10s}: {total_params:,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c363e3",
   "metadata": {},
   "source": [
    "## 3) Why Transfer Learning? The Power of Pretrained Features\n",
    "\n",
    "### The Transfer Learning Hypothesis\n",
    "\n",
    "Networks trained on large datasets (like ImageNet with 1.2M images, 1000 classes) learn hierarchical features:\n",
    "\n",
    "Early layers (conv1, layer1): Low-level features (edges, textures, colors)\n",
    "\n",
    "Middle layers (layer2, layer3): Mid-level features (shapes, parts, patterns)\n",
    "\n",
    "Deep layers (layer4): High-level, task-specific features (object parts)\n",
    "\n",
    "Final layer (fc): Class-specific decision boundaries\n",
    "\n",
    "Key Insight: Low and mid-level features are universal across vision tasks! We can reuse them and only adapt the high-level features to our new task.\n",
    "\n",
    "## Fine-Tuning Strategies\n",
    "\n",
    "### 1) Feature Extraction *(Freeze all, train head)*\n",
    "- **Pros:** Fastest; lowest overfitting risk  \n",
    "- **Use when:** Limited data; domain ≈ ImageNet  \n",
    "- **Unfrozen:** `fc` only\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Shallow Fine-Tuning *(Unfreeze layer4 + head)*\n",
    "- **Pros:** Adapts high-level features; still efficient  \n",
    "- **Use when:** Moderate data; somewhat different domain  \n",
    "- **Unfrozen:** `layer4`, `fc`\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Deep Fine-Tuning *(Unfreeze layer3 + layer4 + head)*\n",
    "- **Pros:** Greater adaptation capacity  \n",
    "- **Use when:** Sufficient data; noticeable domain shift  \n",
    "- **Unfrozen:** `layer3`, `layer4`, `fc`\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Full Fine-Tuning *(Unfreeze everything)*\n",
    "- **Pros:** Maximum flexibility  \n",
    "- **Cons:** Slowest; higher overfitting risk  \n",
    "- **Use when:** Large dataset; very different domain  \n",
    "- **Unfrozen:** all layers\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Tips\n",
    "- Prefer **smaller LR** for earlier layers (discriminative LRs).\n",
    "- Add regularization when unfreezing more (augmentations, weight decay, label smoothing).\n",
    "- Monitor validation; consider early stopping/checkpointing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1c800",
   "metadata": {},
   "source": [
    "## 4) Data Preprocessing: Why ImageNet Statistics?\n",
    "\n",
    "### Understanding ImageNet Normalization\n",
    "Pretrained networks expect inputs with specific statistics because they were trained on normalized ImageNet data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c7c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet channel-wise statistics (computed over millions of images)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # Mean per channel (R, G, B)\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]  # Std dev per channel\n",
    "\n",
    "# Why these specific values?\n",
    "# - They center the data around 0 and scale to ~[-2, 2] range\n",
    "# - This matches the distribution the network was trained on\n",
    "# - Network weights are calibrated to these input scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83273795",
   "metadata": {},
   "source": [
    "## 4.1 Input Size Requirements\n",
    "\n",
    "ResNet-18 was trained on 224×224 crops from ImageNet. However, it can accept various sizes due to:\n",
    "\n",
    "1. Convolutional layers are size-agnostic (they slide across any size)\n",
    "\n",
    "2. Adaptive Average Pooling before fc layer handles any spatial dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62c1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing input size flexibility:\n",
      "  Input: (3×112×112) → Output: torch.Size([1, 1000])\n",
      "  Input: (3×224×224) → Output: torch.Size([1, 1000])\n",
      "  Input: (3×256×256) → Output: torch.Size([1, 1000])\n",
      "  Input: (3×448×448) → Output: torch.Size([1, 1000])\n",
      "\n",
      " Note: While flexible, best performance is near 224×224\n",
      "   (the training resolution)\n"
     ]
    }
   ],
   "source": [
    "# ResNet input size flexibility demonstration\n",
    "def test_input_sizes(model):\n",
    "    \"\"\"Test different input sizes through ResNet\"\"\"\n",
    "    test_sizes = [112, 224, 256, 448]\n",
    "    model.eval()\n",
    "    \n",
    "    print(\" Testing input size flexibility:\")\n",
    "    for size in test_sizes:\n",
    "        x = torch.randn(1, 3, size, size)\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "        print(f\"  Input: (3×{size}×{size}) → Output: {output.shape}\")\n",
    "    \n",
    "    print(\"\\n Note: While flexible, best performance is near 224×224\")\n",
    "    print(\"   (the training resolution)\")\n",
    "\n",
    "test_input_sizes(res18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c22a3",
   "metadata": {},
   "source": [
    "## 4.2 Data Augmentation Philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b79974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " Dataset: 50,000 train, 10,000 val\n",
      " Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224          # Standard ImageNet size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Training transforms: Add variability to prevent overfitting\n",
    "train_tf = transforms.Compose([\n",
    "    # 1. Resize: CIFAR-10 is 32×32, we need 224×224\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    \n",
    "    # 3. Convert to tensor: PIL Image → Tensor, scales to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # 4. Normalize: Match ImageNet statistics\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "    # This does: output = (input - mean) / std\n",
    "])\n",
    "\n",
    "# Validation transforms: No augmentation (we want consistent evaluation)\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True,  download=True, transform=train_tf)\n",
    "val_ds   = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,           # Randomize order each epoch\n",
    "    num_workers=2,          # Parallel data loading\n",
    "    pin_memory=True         # Faster GPU transfer\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,          # Keep validation order consistent\n",
    "    num_workers=2, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "print(f' Dataset: {len(train_ds):,} train, {len(val_ds):,} val')\n",
    "print(f' Classes: {train_ds.classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d56c0",
   "metadata": {},
   "source": [
    "## 5) Model Setup: Adapting ResNet-18 for the toy Task\n",
    "\n",
    "Replacing the Classification Head\n",
    "\n",
    "The pretrained ResNet-18 outputs 1000 classes (ImageNet), but we need 10 (CIFAR-10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5bde5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original FC layer:\n",
      "  Input features: 512\n",
      "  Output features: 1000 (ImageNet classes)\n",
      "\n",
      " New FC layer:\n",
      "  Input features: 512\n",
      "  Output features: 10 (our classes)\n"
     ]
    }
   ],
   "source": [
    "# Start with ImageNet-pretrained weights\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Examine the original classifier\n",
    "print(\" Original FC layer:\")\n",
    "print(f\"  Input features: {model.fc.in_features}\")\n",
    "print(f\"  Output features: {model.fc.out_features} (ImageNet classes)\")\n",
    "\n",
    "# Replace with our custom classifier\n",
    "# The in_features must match (512 for ResNet-18's final feature size)\n",
    "# The NUM_CLASSES will change for other datasets\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "print(\"\\n New FC layer:\")\n",
    "print(f\"  Input features: {model.fc.in_features}\")\n",
    "print(f\"  Output features: {model.fc.out_features} (our classes)\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a03d7b",
   "metadata": {},
   "source": [
    "## Understanding Parameter Names and Hierarchy\n",
    "To selectively freeze/unfreeze layers, we need to understand PyTorch's parameter naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b8ca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Structure (hierarchical view):\n",
      "├── conv1: Conv2d (9,408 params, 9,408 trainable)\n",
      "├── bn1: BatchNorm2d (128 params, 128 trainable)\n",
      "├── relu: ReLU (0 params, 0 trainable)\n",
      "├── maxpool: MaxPool2d (0 params, 0 trainable)\n",
      "├── layer1: Sequential (147,968 params, 147,968 trainable)\n",
      "│   ├── 0: BasicBlock (73,984 params, 73,984 trainable)\n",
      "│   ├── 1: BasicBlock (73,984 params, 73,984 trainable)\n",
      "├── layer2: Sequential (525,568 params, 525,568 trainable)\n",
      "│   ├── 0: BasicBlock (230,144 params, 230,144 trainable)\n",
      "│   ├── 1: BasicBlock (295,424 params, 295,424 trainable)\n",
      "├── layer3: Sequential (2,099,712 params, 2,099,712 trainable)\n",
      "│   ├── 0: BasicBlock (919,040 params, 919,040 trainable)\n",
      "│   ├── 1: BasicBlock (1,180,672 params, 1,180,672 trainable)\n",
      "├── layer4: Sequential (8,393,728 params, 8,393,728 trainable)\n",
      "│   ├── 0: BasicBlock (3,673,088 params, 3,673,088 trainable)\n",
      "│   ├── 1: BasicBlock (4,720,640 params, 4,720,640 trainable)\n",
      "├── avgpool: AdaptiveAvgPool2d (0 params, 0 trainable)\n",
      "├── fc: Linear (5,130 params, 5,130 trainable)\n"
     ]
    }
   ],
   "source": [
    "def explore_model_structure(model, max_depth=2):\n",
    "    \"\"\"Visualize the model's hierarchical structure\"\"\"\n",
    "    \n",
    "    print(\"\\n Model Structure (hierarchical view):\")\n",
    "    \n",
    "    def print_module(module, prefix=\"\", depth=0):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        for name, child in module.named_children():\n",
    "            param_count = sum(p.numel() for p in child.parameters())\n",
    "            trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
    "            print(f\"{prefix}├── {name}: {child.__class__.__name__} \"\n",
    "                  f\"({param_count:,} params, {trainable:,} trainable)\")\n",
    "            if depth < max_depth - 1:\n",
    "                print_module(child, prefix + \"│   \", depth + 1)\n",
    "    \n",
    "    print_module(model)\n",
    "\n",
    "# Explore structure\n",
    "explore_model_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c35761",
   "metadata": {},
   "source": [
    "## 6) Freezing and Unfreezing: The Core Mechanism\n",
    "### How Freezing Works\n",
    "When we \"freeze\" a layer, we set requires_grad=False on its parameters:\n",
    "\n",
    "Frozen parameters: No gradients computed, no updates during backprop\n",
    "\n",
    "Unfrozen parameters: Gradients computed, weights updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c5647d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Freezing entire model...\n",
      "  ResNet: 11,181,642 parameters FROZEN\n",
      "\n",
      " Unfreezing only the FC layer...\n",
      "  Linear: 5,130 parameters UNFROZEN (trainable)\n",
      "\n",
      " Trainable: 5,130 / 11,181,642 parameters (0.045879%)\n"
     ]
    }
   ],
   "source": [
    "def set_requires_grad(module: nn.Module, requires_grad: bool):\n",
    "    \"\"\"\n",
    "    Recursively set requires_grad for all parameters in a module.\n",
    "    \n",
    "    Args:\n",
    "        module: PyTorch module (layer, block, or entire model)\n",
    "        requires_grad: True to unfreeze (train), False to freeze\n",
    "    \"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "    \n",
    "    # Print status\n",
    "    param_count = sum(p.numel() for p in module.parameters())\n",
    "    status = \"UNFROZEN (trainable)\" if requires_grad else \"FROZEN\"\n",
    "    print(f\"  {module.__class__.__name__}: {param_count:,} parameters {status}\")\n",
    "\n",
    "# Example: Freeze entire model, then selectively unfreeze\n",
    "print(\" Freezing entire model...\")\n",
    "set_requires_grad(model, False)\n",
    "\n",
    "print(\"\\n Unfreezing only the FC layer...\")\n",
    "set_requires_grad(model.fc, True)\n",
    "\n",
    "# Verify what's trainable\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n Trainable: {trainable_params:,} / {total_params:,} parameters \"\n",
    "      f\"({100*trainable_params/total_params:.6f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee588b",
   "metadata": {},
   "source": [
    "## 7) Training Infrastructure\n",
    "\n",
    "Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63addda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()  # Enable dropout, batch norm training mode\n",
    "    \n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = running_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "@torch.no_grad()  # Decorator disables gradient computation\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()  # Disable dropout, batch norm eval mode\n",
    "    \n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # Forward pass only (no backward)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d945e",
   "metadata": {},
   "source": [
    "## 8) Phase 1.1: Head-Only Fine-Tuning (Feature Extraction)\n",
    "Strategy: Use ResNet as a Fixed Feature Extractor\n",
    "\n",
    "In this phase, we:\n",
    "\n",
    "1. Freeze all convolutional layers (keep ImageNet features)\n",
    "\n",
    "2. Train only the new classifier head (learn new class boundaries)\n",
    "\n",
    "3. Use higher learning rate (since we're training from scratch)\n",
    "\n",
    "This is the safest approach with limited data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d0ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PHASE 1: HEAD-ONLY FINE-TUNING\n",
      "============================================================\n",
      "\n",
      " Freezing all layers...\n",
      "  ResNet: 11,181,642 parameters FROZEN\n",
      "\n",
      " Unfreezing classifier head...\n",
      "  Linear: 5,130 parameters UNFROZEN (trainable)\n",
      "\n",
      " Optimizer setup:\n",
      "   Learning rate: 0.001\n",
      "   Trainable params: 5,130\n",
      "\n",
      " Training progress:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/u.rd143338/.conda/envs/scrl_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400440653/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 0/782, Loss: 2.4818\n",
      "    Batch 100/782, Loss: 0.9739\n",
      "    Batch 200/782, Loss: 0.9588\n",
      "    Batch 300/782, Loss: 0.7352\n",
      "    Batch 400/782, Loss: 0.9352\n",
      "    Batch 500/782, Loss: 0.8188\n",
      "    Batch 600/782, Loss: 0.6524\n",
      "    Batch 700/782, Loss: 0.5866\n",
      "   Train: Loss=0.8212, Acc=0.734\n",
      "   Val:   Loss=0.6470, Acc=0.777  New best!\n",
      "\n",
      "Epoch 2/3\n",
      "    Batch 0/782, Loss: 0.7357\n",
      "    Batch 100/782, Loss: 0.5079\n",
      "    Batch 200/782, Loss: 0.7547\n",
      "    Batch 300/782, Loss: 0.5252\n",
      "    Batch 400/782, Loss: 0.4904\n",
      "    Batch 500/782, Loss: 0.5411\n",
      "    Batch 600/782, Loss: 0.4878\n",
      "    Batch 700/782, Loss: 0.6596\n",
      "   Train: Loss=0.6179, Acc=0.787\n",
      "   Val:   Loss=0.5880, Acc=0.801  New best!\n",
      "\n",
      "Epoch 3/3\n",
      "    Batch 0/782, Loss: 0.6366\n",
      "    Batch 100/782, Loss: 0.5225\n",
      "    Batch 200/782, Loss: 0.7769\n",
      "    Batch 300/782, Loss: 0.3899\n",
      "    Batch 400/782, Loss: 0.4761\n",
      "    Batch 500/782, Loss: 0.4913\n",
      "    Batch 600/782, Loss: 0.5393\n",
      "    Batch 700/782, Loss: 0.6337\n",
      "   Train: Loss=0.5900, Acc=0.795\n",
      "   Val:   Loss=0.5956, Acc=0.795 \n",
      "\n",
      " Phase 1 Complete!\n",
      "   Best validation accuracy: 0.801\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for Phase 1\n",
    "EPOCHS_HEAD_ONLY = 3    \n",
    "LR_HEAD = 1e-3          \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" PHASE 1: HEAD-ONLY FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Freeze entire model\n",
    "print(\"\\n Freezing all layers...\")\n",
    "set_requires_grad(model, False)\n",
    "\n",
    "# Step 2: Unfreeze only the classifier head\n",
    "print(\"\\n Unfreezing classifier head...\")\n",
    "set_requires_grad(model.fc, True)\n",
    "\n",
    "# Step 3: Create optimizer for ONLY trainable parameters\n",
    "# filter() ensures we only optimize parameters with requires_grad=True\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=LR_HEAD)\n",
    "\n",
    "print(f\"\\n Optimizer setup:\")\n",
    "print(f\"   Learning rate: {LR_HEAD}\")\n",
    "print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Step 4: Training loop\n",
    "print(\"\\n Training progress:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, EPOCHS_HEAD_ONLY + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS_HEAD_ONLY}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Optional: Save best model\n",
    "        # torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
    "    \n",
    "    print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.3f}\")\n",
    "    print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.3f} \"\n",
    "          f\"{' New best!' if val_acc == best_val_acc else ''}\")\n",
    "\n",
    "print(\"\\n Phase 1 Complete!\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7132526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
